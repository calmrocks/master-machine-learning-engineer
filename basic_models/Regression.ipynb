{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCa5VbvbbVTHxE1wY1T5p2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/basic_models/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-section"
      },
      "source": [
        "# Regression Case Study: House Price Prediction\n",
        "\n",
        "## Introduction\n",
        "This notebook demonstrates regression modeling using the California Housing dataset. We'll explore various regression techniques and best practices for predicting house prices.\n",
        "\n",
        "## Dataset Overview\n",
        "The California Housing dataset contains information about houses in California districts, including:\n",
        "- Median house value (target variable)\n",
        "- Housing features (bedrooms, population, income, etc.)\n",
        "- Location information\n",
        "- Economic indicators\n",
        "\n",
        "## Objectives\n",
        "1. Implement and compare different regression models\n",
        "2. Apply feature engineering and selection\n",
        "3. Handle non-linear relationships\n",
        "4. Evaluate model performance\n",
        "5. Demonstrate regression best practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-section"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn')\n",
        "sns.set_theme()\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-loading-section"
      },
      "source": [
        "## Data Loading and Exploration\n",
        "\n",
        "In this section, we'll:\n",
        "1. Load and examine the dataset\n",
        "2. Analyze feature distributions\n",
        "3. Investigate relationships between features\n",
        "4. Check for data quality issues\n",
        "5. Explore target variable distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-data"
      },
      "outputs": [],
      "source": [
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "df['PRICE'] = housing.target\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFeatures:\")\n",
        "print(housing.feature_names)\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nBasic Statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "target-analysis-section"
      },
      "source": [
        "### Target Variable Analysis\n",
        "\n",
        "Let's examine the distribution of house prices and identify any potential issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "target-analysis"
      },
      "outputs": [],
      "source": [
        "# Analyze target variable distribution\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(data=df, x='PRICE', bins=50)\n",
        "plt.title('Distribution of House Prices')\n",
        "plt.xlabel('Price (100k$)')\n",
        "\n",
        "# Box plot\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.boxplot(data=df, y='PRICE')\n",
        "plt.title('Box Plot of House Prices')\n",
        "plt.ylabel('Price (100k$)')\n",
        "\n",
        "# Q-Q plot\n",
        "plt.subplot(1, 3, 3)\n",
        "from scipy import stats\n",
        "stats.probplot(df['PRICE'], dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot of House Prices')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nPrice Statistics:\")\n",
        "print(df['PRICE'].describe())\n",
        "\n",
        "# Check skewness and kurtosis\n",
        "print(\"\\nSkewness:\", df['PRICE'].skew())\n",
        "print(\"Kurtosis:\", df['PRICE'].kurtosis())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-distribution-section"
      },
      "source": [
        "### Feature Distributions\n",
        "\n",
        "Examine the distribution of each feature to identify patterns and potential issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-distributions"
      },
      "outputs": [],
      "source": [
        "# Create distribution plots for all features\n",
        "features = housing.feature_names\n",
        "n_features = len(features)\n",
        "n_rows = (n_features + 1) // 2\n",
        "\n",
        "plt.figure(figsize=(15, 5*n_rows))\n",
        "\n",
        "for idx, feature in enumerate(features, 1):\n",
        "    plt.subplot(n_rows, 2, idx)\n",
        "    \n",
        "    # Histogram with KDE\n",
        "    sns.histplot(data=df, x=feature, bins=50, kde=True)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    \n",
        "    # Add skewness and kurtosis annotations\n",
        "    skew = df[feature].skew()\n",
        "    kurt = df[feature].kurtosis()\n",
        "    plt.annotate(f'Skewness: {skew:.2f}\\nKurtosis: {kurt:.2f}',\n",
        "                 xy=(0.95, 0.95), xycoords='axes fraction',\n",
        "                 ha='right', va='top',\n",
        "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary of feature characteristics\n",
        "print(\"Feature Characteristics:\")\n",
        "for feature in features:\n",
        "    print(f\"\\n{feature}:\")\n",
        "    print(f\"Range: [{df[feature].min():.2f}, {df[feature].max():.2f}]\")\n",
        "    print(f\"Skewness: {df[feature].skew():.2f}\")\n",
        "    print(f\"Kurtosis: {df[feature].kurtosis():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "correlation-analysis-section"
      },
      "source": [
        "### Correlation Analysis\n",
        "\n",
        "Investigate relationships between features and the target variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation-analysis"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "correlation = df.corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print correlations with target variable\n",
        "price_correlations = correlation['PRICE'].sort_values(ascending=False)\n",
        "print(\"\\nCorrelations with Price:\")\n",
        "print(price_correlations)\n",
        "\n",
        "# Scatter plots for top correlated features\n",
        "top_features = price_correlations[1:5].index  # Exclude PRICE itself\n",
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "for idx, feature in enumerate(top_features, 1):\n",
        "    plt.subplot(1, 4, idx)\n",
        "    plt.scatter(df[feature], df['PRICE'], alpha=0.5)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('PRICE')\n",
        "    plt.title(f'Price vs {feature}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-interactions-section"
      },
      "source": [
        "### Feature Interactions\n",
        "\n",
        "Explore interactions between important features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-interactions"
      },
      "outputs": [],
      "source": [
        "# Create pair plots for top correlated features\n",
        "top_features_with_price = list(top_features) + ['PRICE']\n",
        "sns.pairplot(df[top_features_with_price], diag_kind='kde')\n",
        "plt.suptitle('Pair Plot of Top Features', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 3D scatter plot for top 2 features vs price\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(df[top_features[0]], \n",
        "                    df[top_features[1]], \n",
        "                    df['PRICE'],\n",
        "                    c=df['PRICE'],\n",
        "                    cmap='viridis')\n",
        "\n",
        "ax.set_xlabel(top_features[0])\n",
        "ax.set_ylabel(top_features[1])\n",
        "ax.set_zlabel('PRICE')\n",
        "plt.colorbar(scatter)\n",
        "plt.title('3D Visualization of Top Features vs Price')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initial-findings"
      },
      "source": [
        "### Initial Findings\n",
        "\n",
        "1. **Target Variable (Price)**:\n",
        "   - Distribution characteristics\n",
        "   - Presence of outliers\n",
        "   - Potential need for transformation\n",
        "\n",
        "2. **Feature Characteristics**:\n",
        "   - Range and scale differences\n",
        "   - Skewness in distributions\n",
        "   - Potential outliers\n",
        "\n",
        "3. **Relationships**:\n",
        "   - Strong correlations identified\n",
        "   - Non-linear patterns\n",
        "   - Important feature interactions\n",
        "\n",
        "4. **Data Quality**:\n",
        "   - Missing values assessment\n",
        "   - Outlier impact\n",
        "   - Feature scaling needs\n",
        "\n",
        "Next steps:\n",
        "1. Feature engineering based on observed patterns\n",
        "2. Handle skewed distributions\n",
        "3. Address outliers\n",
        "4. Prepare data for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-engineering-section"
      },
      "source": [
        "## Feature Engineering and Preprocessing\n",
        "\n",
        "Based on our exploratory analysis, we'll:\n",
        "1. Handle skewed features and outliers\n",
        "2. Create interaction features\n",
        "3. Apply feature transformations\n",
        "4. Scale features appropriately\n",
        "5. Prepare data for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "handle-skewness-section"
      },
      "source": [
        "### Handling Skewed Features\n",
        "\n",
        "Apply appropriate transformations to handle skewed distributions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skewness-transformation"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "def analyze_skewness(data, threshold=0.5):\n",
        "    \"\"\"Analyze feature skewness and suggest transformations.\"\"\"\n",
        "    skew_stats = pd.DataFrame({\n",
        "        'Original_Skew': data.skew()\n",
        "    })\n",
        "    \n",
        "    # Try different transformations\n",
        "    for feature in data.columns:\n",
        "        # Log transformation (adding 1 to handle zeros)\n",
        "        if (data[feature] >= 0).all():\n",
        "            log_skew = np.log1p(data[feature]).skew()\n",
        "            skew_stats.loc[feature, 'Log_Skew'] = log_skew\n",
        "        \n",
        "        # Square root transformation\n",
        "        if (data[feature] >= 0).all():\n",
        "            sqrt_skew = np.sqrt(data[feature]).skew()\n",
        "            skew_stats.loc[feature, 'Sqrt_Skew'] = sqrt_skew\n",
        "        \n",
        "        # Box-Cox transformation\n",
        "        if (data[feature] > 0).all():\n",
        "            boxcox_data, _ = stats.boxcox(data[feature])\n",
        "            boxcox_skew = pd.Series(boxcox_data).skew()\n",
        "            skew_stats.loc[feature, 'BoxCox_Skew'] = boxcox_skew\n",
        "    \n",
        "    return skew_stats\n",
        "\n",
        "# Analyze skewness\n",
        "skew_analysis = analyze_skewness(df)\n",
        "print(\"Skewness Analysis:\")\n",
        "display(skew_analysis)\n",
        "\n",
        "# Apply transformations to skewed features\n",
        "df_transformed = df.copy()\n",
        "transformations = {}\n",
        "\n",
        "for feature in df.columns:\n",
        "    if abs(df[feature].skew()) > 0.5:  # Apply transformation if skewness > 0.5\n",
        "        if (df[feature] >= 0).all():\n",
        "            # Choose best transformation based on skewness reduction\n",
        "            skew_values = skew_analysis.loc[feature].dropna()\n",
        "            best_transform = skew_values.abs().idxmin()\n",
        "            \n",
        "            if 'Log' in best_transform:\n",
        "                df_transformed[f'{feature}_transformed'] = np.log1p(df[feature])\n",
        "                transformations[feature] = 'log'\n",
        "            elif 'Sqrt' in best_transform:\n",
        "                df_transformed[f'{feature}_transformed'] = np.sqrt(df[feature])\n",
        "                transformations[feature] = 'sqrt'\n",
        "            elif 'BoxCox' in best_transform and (df[feature] > 0).all():\n",
        "                df_transformed[f'{feature}_transformed'], _ = stats.boxcox(df[feature])\n",
        "                transformations[feature] = 'boxcox'\n",
        "\n",
        "# Visualize transformations\n",
        "for feature, transform in transformations.items():\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Original distribution\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(data=df, x=feature, bins=50, kde=True)\n",
        "    plt.title(f'Original {feature} Distribution')\n",
        "    \n",
        "    # Transformed distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(data=df_transformed, x=f'{feature}_transformed', bins=50, kde=True)\n",
        "    plt.title(f'Transformed {feature} Distribution ({transform})')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interaction-features-section"
      },
      "source": [
        "### Feature Interactions\n",
        "\n",
        "Create meaningful interaction features based on domain knowledge and correlations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-interactions"
      },
      "outputs": [],
      "source": [
        "# Create interaction features\n",
        "def create_interactions(df, top_n=3):\n",
        "    \"\"\"Create interaction features from top correlated features.\"\"\"\n",
        "    # Get top correlated features with price\n",
        "    correlations = df.corr()['PRICE'].abs().sort_values(ascending=False)\n",
        "    top_features = correlations[1:top_n+1].index\n",
        "    \n",
        "    df_interactions = df.copy()\n",
        "    \n",
        "    # Create multiplicative interactions\n",
        "    for i in range(len(top_features)):\n",
        "        for j in range(i+1, len(top_features)):\n",
        "            feat1, feat2 = top_features[i], top_features[j]\n",
        "            interaction_name = f'{feat1}_{feat2}_interaction'\n",
        "            df_interactions[interaction_name] = df[feat1] * df[feat2]\n",
        "    \n",
        "    # Create ratio features\n",
        "    for i in range(len(top_features)):\n",
        "        for j in range(len(top_features)):\n",
        "            if i != j:\n",
        "                feat1, feat2 = top_features[i], top_features[j]\n",
        "                ratio_name = f'{feat1}_per_{feat2}'\n",
        "                df_interactions[ratio_name] = df[feat1] / (df[feat2] + 1e-6)\n",
        "    \n",
        "    return df_interactions\n",
        "\n",
        "# Create interaction features\n",
        "df_with_interactions = create_interactions(df_transformed)\n",
        "\n",
        "# Analyze new features\n",
        "new_features = [col for col in df_with_interactions.columns \n",
        "               if col not in df_transformed.columns]\n",
        "\n",
        "# Calculate correlations with price for new features\n",
        "new_correlations = df_with_interactions[new_features + ['PRICE']].corr()['PRICE']\n",
        "print(\"Correlations of new features with price:\")\n",
        "display(new_correlations.sort_values(ascending=False))\n",
        "\n",
        "# Visualize top new features\n",
        "top_new_features = new_correlations.abs().sort_values(ascending=False).head(4).index\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "for idx, feature in enumerate(top_new_features, 1):\n",
        "    plt.subplot(1, 4, idx)\n",
        "    plt.scatter(df_with_interactions[feature], \n",
        "               df_with_interactions['PRICE'], \n",
        "               alpha=0.5)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('PRICE')\n",
        "    plt.title(f'Price vs {feature}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "polynomial-features-section"
      },
      "source": [
        "### Polynomial Features\n",
        "\n",
        "Create polynomial features to capture non-linear relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-polynomial"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def create_polynomial_features(df, features, degree=2):\n",
        "    \"\"\"Create polynomial features for specified columns.\"\"\"\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    poly_features = poly.fit_transform(df[features])\n",
        "    \n",
        "    # Generate feature names\n",
        "    feature_names = poly.get_feature_names_out(features)\n",
        "    \n",
        "    # Create DataFrame with polynomial features\n",
        "    df_poly = pd.DataFrame(poly_features, columns=feature_names)\n",
        "    \n",
        "    # Add non-polynomial columns\n",
        "    for col in df.columns:\n",
        "        if col not in features:\n",
        "            df_poly[col] = df[col]\n",
        "    \n",
        "    return df_poly\n",
        "\n",
        "# Select features for polynomial transformation\n",
        "poly_features = df.corr()['PRICE'].abs().sort_values(ascending=False)[1:4].index\n",
        "\n",
        "# Create polynomial features\n",
        "df_with_poly = create_polynomial_features(df_with_interactions, poly_features)\n",
        "\n",
        "# Analyze new polynomial features\n",
        "new_poly_features = [col for col in df_with_poly.columns \n",
        "                    if col not in df_with_interactions.columns]\n",
        "\n",
        "# Calculate correlations with price for polynomial features\n",
        "poly_correlations = df_with_poly[new_poly_features + ['PRICE']].corr()['PRICE']\n",
        "print(\"Correlations of polynomial features with price:\")\n",
        "display(poly_correlations.sort_values(ascending=False))\n",
        "\n",
        "# Visualize top polynomial features\n",
        "top_poly_features = poly_correlations.abs().sort_values(ascending=False).head(4).index\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "for idx, feature in enumerate(top_poly_features, 1):\n",
        "    plt.subplot(1, 4, idx)\n",
        "    plt.scatter(df_with_poly[feature], \n",
        "               df_with_poly['PRICE'], \n",
        "               alpha=0.5)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('PRICE')\n",
        "    plt.title(f'Price vs {feature}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-selection-section"
      },
      "source": [
        "### Feature Selection\n",
        "\n",
        "Select most important features using various methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-selection"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "\n",
        "def select_features(X, y, n_features=20):\n",
        "    \"\"\"Select top features using multiple methods.\"\"\"\n",
        "    # Correlation based selection\n",
        "    correlations = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'correlation': X.corrwith(y).abs()\n",
        "    }).sort_values('correlation', ascending=False)\n",
        "    \n",
        "    # F-regression based selection\n",
        "    f_selector = SelectKBest(f_regression, k=n_features)\n",
        "    f_selector.fit(X, y)\n",
        "    f_scores = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'f_score': f_selector.scores_\n",
        "    }).sort_values('f_score', ascending=False)\n",
        "    \n",
        "    # Mutual information based selection\n",
        "    mi_selector = SelectKBest(mutual_info_regression, k=n_features)\n",
        "    mi_selector.fit(X, y)\n",
        "    mi_scores = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'mi_score': mi_selector.scores_\n",
        "    }).sort_values('mi_score', ascending=False)\n",
        "    \n",
        "    return correlations, f_scores, mi_scores\n",
        "\n",
        "# Prepare data for feature selection\n",
        "X = df_with_poly.drop('PRICE', axis=1)\n",
        "y = df_with_poly['PRICE']\n",
        "\n",
        "# Select features\n",
        "corr_features, f_features, mi_features = select_features(X, y)\n",
        "\n",
        "# Plot feature importance scores\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Correlation based importance\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.barplot(data=corr_features.head(10), x='correlation', y='feature')\n",
        "plt.title('Top Features by Correlation')\n",
        "\n",
        "# F-score based importance\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.barplot(data=f_features.head(10), x='f_score', y='feature')\n",
        "plt.title('Top Features by F-Score')\n",
        "\n",
        "# Mutual information based importance\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.barplot(data=mi_features.head(10), x='mi_score', y='feature')\n",
        "plt.title('Top Features by Mutual Information')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select final features that appear in top 10 of at least 2 methods\n",
        "top_features_sets = [\n",
        "    set(corr_features.head(10)['feature']),\n",
        "    set(f_features.head(10)['feature']),\n",
        "    set(mi_features.head(10)['feature'])\n",
        "]\n",
        "\n",
        "final_features = set()\n",
        "for feature in X.columns:\n",
        "    if sum([feature in feature_set for feature_set in top_features_sets]) >= 2:\n",
        "        final_features.add(feature)\n",
        "\n",
        "print(\"\\nSelected features:\")\n",
        "print(final_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scaling-section"
      },
      "source": [
        "### Feature Scaling\n",
        "\n",
        "Scale the selected features for modeling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-scaling"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "# Prepare final feature set\n",
        "X_final = df_with_poly[list(final_features)]\n",
        "y_final = df_with_poly['PRICE']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_final, y_final, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert to DataFrame to maintain feature names\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Display scaling results\n",
        "print(\"Scaled features statistics:\")\n",
        "display(X_train_scaled.describe())\n",
        "\n",
        "# Save preprocessed data\n",
        "preprocessed_data = {\n",
        "    'X_train': X_train_scaled,\n",
        "    'X_test': X_test_scaled,\n",
        "    'y_train': y_train,\n",
        "    'y_test': y_test,\n",
        "    'scaler': scaler,\n",
        "    'feature_names': list(final_features)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing-summary"
      },
      "source": [
        "### Preprocessing Summary\n",
        "\n",
        "1. **Feature Transformations**:\n",
        "   - Handled skewed features\n",
        "   - Created interaction features\n",
        "   - Added polynomial terms\n",
        "\n",
        "2. **Feature Selection**:\n",
        "   - Used multiple selection methods\n",
        "   - Combined results for robust selection\n",
        "   - Identified most important features\n",
        "\n",
        "3. **Data Preparation**:\n",
        "   - Scaled features appropriately\n",
        "   - Split data for modeling\n",
        "   - Preserved feature names\n",
        "\n",
        "Next steps:\n",
        "1. Implement regression models\n",
        "2. Compare model performance\n",
        "3. Fine-tune best models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-implementation-section"
      },
      "source": [
        "## Model Implementation\n",
        "\n",
        "We'll implement several regression models:\n",
        "1. Linear Regression (baseline)\n",
        "2. Ridge and Lasso Regression\n",
        "3. Random Forest\n",
        "4. XGBoost\n",
        "5. LightGBM\n",
        "\n",
        "For each model, we'll:\n",
        "- Train with default parameters\n",
        "- Make predictions\n",
        "- Calculate regression metrics\n",
        "- Analyze residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-utils"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def evaluate_regression_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Calculate and display regression metrics.\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "    \n",
        "    # Plot actual vs predicted\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Actual vs Predicted\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual Price')\n",
        "    plt.ylabel('Predicted Price')\n",
        "    plt.title(f'{model_name}: Actual vs Predicted')\n",
        "    \n",
        "    # Residuals\n",
        "    residuals = y_true - y_pred\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Predicted Price')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Residual Plot')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'r2': r2\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linear-regression-section"
      },
      "source": [
        "### Linear Regression (Baseline)\n",
        "\n",
        "Implement basic linear regression as a baseline model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "linear-regression"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train linear regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "lr_metrics = evaluate_regression_model(y_test, lr_pred, 'Linear Regression')\n",
        "\n",
        "# Feature importance\n",
        "lr_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'coefficient': abs(lr_model.coef_)\n",
        "}).sort_values('coefficient', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=lr_importance.head(10), x='coefficient', y='feature')\n",
        "plt.title('Top 10 Features (Linear Regression)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regularized-regression-section"
      },
      "source": [
        "### Ridge and Lasso Regression\n",
        "\n",
        "Implement regularized regression models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "regularized-regression"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "ridge_pred = ridge_model.predict(X_test_scaled)\n",
        "ridge_metrics = evaluate_regression_model(y_test, ridge_pred, 'Ridge Regression')\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso(alpha=1.0)\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "lasso_pred = lasso_model.predict(X_test_scaled)\n",
        "lasso_metrics = evaluate_regression_model(y_test, lasso_pred, 'Lasso Regression')\n",
        "\n",
        "# Compare feature importance\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Ridge coefficients\n",
        "plt.subplot(1, 2, 1)\n",
        "ridge_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'coefficient': abs(ridge_model.coef_)\n",
        "}).sort_values('coefficient', ascending=False)\n",
        "sns.barplot(data=ridge_importance.head(10), x='coefficient', y='feature')\n",
        "plt.title('Top 10 Features (Ridge)')\n",
        "\n",
        "# Lasso coefficients\n",
        "plt.subplot(1, 2, 2)\n",
        "lasso_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'coefficient': abs(lasso_model.coef_)\n",
        "}).sort_values('coefficient', ascending=False)\n",
        "sns.barplot(data=lasso_importance.head(10), x='coefficient', y='feature')\n",
        "plt.title('Top 10 Features (Lasso)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "random-forest-section"
      },
      "source": [
        "### Random Forest Regression\n",
        "\n",
        "Implement Random Forest model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "random-forest"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "rf_pred = rf_model.predict(X_test_scaled)\n",
        "rf_metrics = evaluate_regression_model(y_test, rf_pred, 'Random Forest')\n",
        "\n",
        "# Feature importance\n",
        "rf_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=rf_importance.head(10), x='importance', y='feature')\n",
        "plt.title('Top 10 Features (Random Forest)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgboost-section"
      },
      "source": [
        "### XGBoost Regression\n",
        "\n",
        "Implement XGBoost model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgboost"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test_scaled)\n",
        "xgb_metrics = evaluate_regression_model(y_test, xgb_pred, 'XGBoost')\n",
        "\n",
        "# Feature importance\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'importance': xgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=xgb_importance.head(10), x='importance', y='feature')\n",
        "plt.title('Top 10 Features (XGBoost)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lightgbm-section"
      },
      "source": [
        "### LightGBM Regression\n",
        "\n",
        "Implement LightGBM model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lightgbm"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Train LightGBM\n",
        "lgb_model = LGBMRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=31,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "lgb_pred = lgb_model.predict(X_test_scaled)\n",
        "lgb_metrics = evaluate_regression_model(y_test, lgb_pred, 'LightGBM')\n",
        "\n",
        "# Feature importance\n",
        "lgb_importance = pd.DataFrame({\n",
        "    'feature': X_train_scaled.columns,\n",
        "    'importance': lgb_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=lgb_importance.head(10), x='importance', y='feature')\n",
        "plt.title('Top 10 Features (LightGBM)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-comparison-section"
      },
      "source": [
        "### Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-comparison"
      },
      "outputs": [],
      "source": [
        "# Collect all results\n",
        "models = {\n",
        "    'Linear Regression': lr_metrics,\n",
        "    'Ridge': ridge_metrics,\n",
        "    'Lasso': lasso_metrics,\n",
        "    'Random Forest': rf_metrics,\n",
        "    'XGBoost': xgb_metrics,\n",
        "    'LightGBM': lgb_metrics\n",
        "}\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame(models).T\n",
        "print(\"Model Comparison:\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Plot metrics comparison\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# RMSE\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.barplot(data=comparison_df.reset_index(), x='index', y='rmse')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('RMSE by Model')\n",
        "\n",
        "# MAE\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.barplot(data=comparison_df.reset_index(), x='index', y='mae')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('MAE by Model')\n",
        "\n",
        "# R2\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.barplot(data=comparison_df.reset_index(), x='index', y='r2')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('R² Score by Model')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "implementation-summary"
      },
      "source": [
        "### Model Implementation Summary\n",
        "\n",
        "1. **Model Performance**:\n",
        "   - Best performing model: [based on results]\n",
        "   - Performance metrics comparison\n",
        "   - Residual analysis insights\n",
        "\n",
        "2. **Feature Importance**:\n",
        "   - Consistent important features across models\n",
        "   - Different importance rankings\n",
        "   - Feature selection validation\n",
        "\n",
        "3. **Model Characteristics**:\n",
        "   - Linear vs non-linear performance\n",
        "   - Regularization effects\n",
        "   - Ensemble method benefits\n",
        "\n",
        "Next steps:\n",
        "1. Hyperparameter tuning\n",
        "2. Model stacking/ensembling\n",
        "3. Cross-validation analysis"
      ]
    }
  ]
}