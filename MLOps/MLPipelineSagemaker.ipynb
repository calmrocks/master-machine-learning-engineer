{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/MLOps/MLPipelineSagemaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Machine Learning Pipeline with Amazon SageMaker\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build an automated ML pipeline using Amazon SageMaker. We'll showcase:\n",
        "- Automated data processing using SageMaker Processing Jobs\n",
        "- Model training using SageMaker Training Jobs\n",
        "- Model deployment using SageMaker Endpoints\n",
        "- Automated monitoring using Model Monitor\n",
        "- Automated retraining using SageMaker Pipelines\n",
        "\n",
        "![ML Pipeline](https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/Diagrams/MLPipeline.png?raw=1)"
      ],
      "metadata": {
        "id": "bpyWlVSZr7Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to implement an AWS SageMaker ML pipeline in Google Colab. Before we proceed with the pipeline implementation, we need to set up AWS credentials.\n",
        "\n",
        "### Credential Setup Method\n",
        "We'll use interactive forms to securely input AWS credentials. This method:\n",
        "- Keeps credentials temporary (only for current session)\n",
        "- Avoids storing sensitive information in the notebook\n",
        "- Uses password fields to hide sensitive input\n",
        "- Clears the form after credentials are set\n",
        "\n",
        "### Required AWS Information\n",
        "You'll need the following information ready:\n",
        "1. **AWS Access Key ID**: Your AWS account access key\n",
        "2. **AWS Secret Access Key**: Your AWS account secret key\n",
        "3. **AWS Region**: The AWS region you want to work in (e.g., 'us-east-1')\n",
        "4. **S3 Bucket**: The name of your S3 bucket for storing pipeline artifacts\n",
        "5. **Role ARN**: The Amazon Resource Name of your IAM role with SageMaker permissions\n",
        "\n",
        "### Prerequisites\n",
        "Make sure you have:\n",
        "- An active AWS account\n",
        "- IAM user with appropriate permissions\n",
        "- S3 bucket created\n",
        "- IAM role configured for SageMaker\n",
        "\n",
        "Run the following cell to set up your credentials:\n"
      ],
      "metadata": {
        "id": "ajz2nIifr-Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 sagemaker ipywidgets s3fs"
      ],
      "metadata": {
        "id": "RC0Z0pqHP5e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting AWS Credentials\n",
        "\n",
        "There are several ways to obtain and use AWS credentials depending on your setup:\n",
        "\n",
        "### If Using Amazon SageMaker Notebook Instance\n",
        "\n",
        "If you're running this notebook in a SageMaker notebook instance, you can leverage the instance's built-in credentials:\n",
        "\n",
        "```python\n",
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "# Get the default SageMaker session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Get the role ARN\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "# Get the default bucket\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "\n",
        "# Get the boto3 session\n",
        "session = boto3.Session()\n",
        "\n",
        "# Print details\n",
        "print(f\"Role ARN: {role}\")\n",
        "print(f\"Default bucket: {default_bucket}\")\n",
        "```"
      ],
      "metadata": {
        "id": "rVx9tUh5u-f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "import sagemaker\n",
        "import logging\n",
        "from IPython.display import clear_output\n",
        "\n",
        "access_key = input(\"AWS Access Key ID: \")\n",
        "secret_key = input(\"AWS Secret Access Key: \")\n",
        "session_token = input(\"AWS Session Token (press Enter if none): \").strip() or None\n",
        "region = input(\"AWS Region (default: us-east-1): \") or \"us-east-1\"\n",
        "bucket = input(\"S3 Bucket Name: \")\n",
        "role_arn = input(\"Role ARN: \")\n",
        "\n",
        "print(\"\\nCredentials set:\")\n",
        "print(f\"Access Key: {access_key[:4]}...{access_key[-4:]}\")\n",
        "print(f\"Secret Key: {secret_key[:4]}...{secret_key[-4:]}\")\n",
        "if session_token:\n",
        "    print(f\"Session Token: {session_token[:4]}...{session_token[-4:]}\")\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"Bucket: {bucket}\")\n",
        "print(f\"Role ARN: {role_arn}\\n\")\n",
        "\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=access_key,\n",
        "    aws_secret_access_key=secret_key,\n",
        "    aws_session_token=session_token,\n",
        "    region_name=region\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "s3 = session.client('s3')\n",
        "bucket_name = bucket\n",
        "try:\n",
        "    s3.head_bucket(Bucket=bucket_name)\n",
        "    print(f\"✓ Successfully accessed S3 bucket: {bucket_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error accessing S3 bucket: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLlyb5wpPlkH",
        "outputId": "5a25338a-00a4-404d-b7e8-fa3e193fc12d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully accessed S3 bucket: yinglonw-test-us-east-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Project Files\n",
        "\n",
        "Before we start building our ML pipeline, we need to get our project files from GitHub. These files contain:\n",
        "\n",
        "- `pipeline.py`: Main pipeline configuration that orchestrates the ML workflow\n",
        "- `scripts/preprocessing.py`: Data preprocessing logic including cleaning and feature engineering\n",
        "- `scripts/training.py`: Model training code using Random Forest algorithm\n",
        "- `scripts/evaluation.py`: Model evaluation metrics calculation\n",
        "\n",
        "The code below will:\n",
        "1. Download these files from GitHub repository\n",
        "2. Create necessary directories in our notebook environment\n",
        "3. Save the files locally so we can use them in our pipeline\n",
        "\n",
        "This setup ensures we have all required scripts available in our SageMaker notebook instance to build and execute our ML pipeline."
      ],
      "metadata": {
        "id": "_Jqdur2YD1Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_github_file(github_url, local_path):\n",
        "    \"\"\"\n",
        "    Download a file from GitHub and save it locally.\n",
        "    Converts GitHub web URL to raw content URL.\n",
        "    \"\"\"\n",
        "    # Convert GitHub URL to raw content URL\n",
        "    raw_url = github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "    # Download and save the file\n",
        "    response = requests.get(raw_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_path, 'w') as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Successfully downloaded: {local_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download: {local_path}\")\n",
        "        print(f\"Status code: {response.status_code}\")\n",
        "\n",
        "# Define the files to download\n",
        "files = {\n",
        "    'pipeline.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/pipeline.py',\n",
        "    'scripts/preprocessing.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/preprocessing.py',\n",
        "    'scripts/training.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/training.py',\n",
        "    'scripts/evaluation.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/evaluation.py'\n",
        "}\n",
        "\n",
        "# Download all files\n",
        "for local_path, github_url in files.items():\n",
        "    download_github_file(github_url, f'wine_quality_pipeline/{local_path}')\n",
        "\n",
        "print(\"\\nChecking downloaded files:\")\n",
        "print(list(Path(\"wine_quality_pipeline\").glob(\"**/*.py\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxQBgMCUD1rF",
        "outputId": "7b1400d6-085a-4c21-f780-0f3bc123f46a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded: wine_quality_pipeline/pipeline.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/preprocessing.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/training.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/evaluation.py\n",
            "\n",
            "Checking downloaded files:\n",
            "[PosixPath('wine_quality_pipeline/pipeline.py'), PosixPath('wine_quality_pipeline/scripts/evaluation.py'), PosixPath('wine_quality_pipeline/scripts/preprocessing.py'), PosixPath('wine_quality_pipeline/scripts/training.py')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wine Quality ML Pipeline with Amazon SageMaker\n",
        "\n",
        "This notebook demonstrates how to build an end-to-end machine learning pipeline using Amazon SageMaker. We'll use the Wine Quality dataset to showcase:\n",
        "- Data preprocessing\n",
        "- Model training\n",
        "- Model evaluation\n",
        "- Automated retraining\n",
        "- Model monitoring\n",
        "\n",
        "The pipeline will automatically handle data preprocessing, model training, and evaluation, making it easy to retrain models when new data arrives."
      ],
      "metadata": {
        "id": "PrTTPpNCBwac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Import Required Libraries\n",
        "\n",
        "First, let's import our required libraries and setup our project structure."
      ],
      "metadata": {
        "id": "blY-Gi6bDUMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our pipeline creation function\n",
        "from wine_quality_pipeline.pipeline import create_pipeline\n",
        "\n",
        "print(\"Current working directory:\", Path.cwd())\n",
        "print(\"\\nContents of wine_quality_pipeline:\")\n",
        "print(list(Path(\"wine_quality_pipeline\").glob(\"**/*.py\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZXt0YxsDWDA",
        "outputId": "d16fe051-5fd6-48e0-9bb6-619229ba0600"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "\n",
            "Contents of wine_quality_pipeline:\n",
            "[PosixPath('wine_quality_pipeline/pipeline.py'), PosixPath('wine_quality_pipeline/scripts/evaluation.py'), PosixPath('wine_quality_pipeline/scripts/preprocessing.py'), PosixPath('wine_quality_pipeline/scripts/training.py')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Prepare Initial Dataset\n",
        "\n",
        "First, let's download the Wine Quality dataset and upload it to our S3 bucket. We'll use this as our initial training data."
      ],
      "metadata": {
        "id": "gxEPMErNBzD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import io\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Download wine quality dataset\n",
        "wine_data = pd.read_csv(\n",
        "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
        "    sep=';'\n",
        ")\n",
        "\n",
        "# Define S3 path\n",
        "initial_data_path = f\"s3://{bucket}/wine-quality/raw/winequality.csv\"\n",
        "\n",
        "# Convert DataFrame to CSV buffer\n",
        "csv_buffer = io.StringIO()\n",
        "wine_data.to_csv(csv_buffer, index=False)\n",
        "\n",
        "# Parse S3 URL to get bucket and key\n",
        "def parse_s3_url(s3_url):\n",
        "    parsed = urlparse(s3_url)\n",
        "    bucket = parsed.netloc\n",
        "    key = parsed.path.lstrip('/')\n",
        "    return bucket, key\n",
        "\n",
        "bucket_name, key_path = parse_s3_url(initial_data_path)\n",
        "\n",
        "# Upload using the session's S3 client\n",
        "s3 = session.client('s3')\n",
        "s3.put_object(\n",
        "    Bucket=bucket_name,\n",
        "    Key=key_path,\n",
        "    Body=csv_buffer.getvalue()\n",
        ")\n",
        "\n",
        "print(f\"Data uploaded to: {initial_data_path}\")\n",
        "print(f\"Dataset shape: {wine_data.shape}\")\n",
        "print(\"\\nFeatures:\")\n",
        "print(wine_data.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OjxsB57Bw81",
        "outputId": "04eee863-afc5-41b1-bfc6-f61463c4a425"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data uploaded to: s3://yinglonw-test-us-east-1/wine-quality/raw/winequality.csv\n",
            "Dataset shape: (1599, 12)\n",
            "\n",
            "Features:\n",
            "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Scripts to S3\n",
        "\n",
        "Now, let's upload our existing preprocessing, training, and evaluation scripts to S3."
      ],
      "metadata": {
        "id": "bs94jGAYExgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_scripts_to_s3(boto3_session, bucket, prefix=\"wine-quality/code\"):\n",
        "    # Get S3 client from boto3 session\n",
        "    s3_client = boto3_session.client('s3')\n",
        "\n",
        "    # Define the scripts to upload\n",
        "    scripts = {\n",
        "        'preprocessing.py': 'wine_quality_pipeline/scripts/preprocessing.py',\n",
        "        'training.py': 'wine_quality_pipeline/scripts/training.py',\n",
        "        'evaluation.py': 'wine_quality_pipeline/scripts/evaluation.py'\n",
        "    }\n",
        "\n",
        "    # Upload each script\n",
        "    for s3_key, local_path in scripts.items():\n",
        "        try:\n",
        "            s3_client.upload_file(\n",
        "                local_path,\n",
        "                bucket,\n",
        "                f\"{prefix}/{s3_key}\"\n",
        "            )\n",
        "            print(f\"Uploaded {local_path} to s3://{bucket}/{prefix}/{s3_key}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
        "\n",
        "    return f\"s3://{bucket}/{prefix}\"\n",
        "\n",
        "# Upload scripts using the boto3 session\n",
        "script_prefix = upload_scripts_to_s3(session, bucket)\n",
        "print(\"\\nAll scripts uploaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8pq4wvIB21s",
        "outputId": "959c3572-19e0-4680-b40f-097847fdda97"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded wine_quality_pipeline/scripts/preprocessing.py to s3://yinglonw-test-us-east-1/wine-quality/code/preprocessing.py\n",
            "Uploaded wine_quality_pipeline/scripts/training.py to s3://yinglonw-test-us-east-1/wine-quality/code/training.py\n",
            "Uploaded wine_quality_pipeline/scripts/evaluation.py to s3://yinglonw-test-us-east-1/wine-quality/code/evaluation.py\n",
            "\n",
            "All scripts uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Configure Pipeline\n",
        "\n",
        "Now we'll create our SageMaker pipeline using our existing pipeline configuration."
      ],
      "metadata": {
        "id": "WRSt4u9RFXxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import wine_quality_pipeline.pipeline\n",
        "importlib.reload(wine_quality_pipeline.pipeline)\n",
        "from wine_quality_pipeline.pipeline import create_pipeline"
      ],
      "metadata": {
        "id": "DmCx1cBuSmVW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipeline, model_monitor = create_pipeline(\n",
        "    role=role_arn,\n",
        "    bucket=bucket,\n",
        "    pipeline_name=\"WineQualityPipeline\",\n",
        "    base_job_prefix=\"wine-quality\",\n",
        "    boto3_session=session\n",
        ")\n",
        "\n",
        "print(\"Pipeline created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjjGRLcjEzsI",
        "outputId": "e0a4be4d-585d-48de-9710-33ec4efc8435"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Model Monitoring\n",
        "\n",
        "Configure monitoring for our model to track its performance over time."
      ],
      "metadata": {
        "id": "5lmprYLVGCsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_cloudwatch_alerts(cloudwatch_client):\n",
        "    \"\"\"Setup CloudWatch alerts for model monitoring\"\"\"\n",
        "    try:\n",
        "        # Example alert for model performance degradation\n",
        "        cloudwatch_client.put_metric_alarm(\n",
        "            AlarmName='WineQualityModelDegradation',\n",
        "            MetricName='mse',\n",
        "            Namespace='WineQualityModel',\n",
        "            Statistic='Average',\n",
        "            Period=300,\n",
        "            EvaluationPeriods=2,\n",
        "            Threshold=0.5,\n",
        "            ComparisonOperator='GreaterThanThreshold'\n",
        "            # Add AlarmActions to get notified\n",
        "        )\n",
        "        print(\"CloudWatch alert created successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating CloudWatch alert: {str(e)}\")\n",
        "\n",
        "# Setup CloudWatch alerts\n",
        "cloudwatch = session.client('cloudwatch')\n",
        "setup_cloudwatch_alerts(cloudwatch)\n",
        "\n",
        "print(\"Model monitoring setup complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuGMKzPFFagI",
        "outputId": "eed0c97a-ad09-4580-b27d-6283ee98297a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CloudWatch alert created successfully!\n",
            "Model monitoring setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Pipeline\n",
        "\n",
        "Finally, let's execute our pipeline and start monitoring."
      ],
      "metadata": {
        "id": "-VILXqdPGGNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline\n",
        "pipeline.upsert(role_arn=role_arn)\n",
        "execution = pipeline.start()\n",
        "\n",
        "print(f\"Pipeline execution started with ARN: {execution.arn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvTRXF7ZGIB3",
        "outputId": "95449579-9fe3-436f-d388-c08449821c13"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
            "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
            "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
            "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
            "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
            "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline execution started with ARN: arn:aws:sagemaker:us-east-1:191514433341:pipeline/WineQualityPipeline/execution/dmw6rirzcmmn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitor Pipeline Execution\n",
        "\n",
        "Let's check the status of our pipeline execution."
      ],
      "metadata": {
        "id": "GBUaJiGnGJxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pipeline_status(execution):\n",
        "    \"\"\"Monitor the pipeline execution status\"\"\"\n",
        "    print(f\"Pipeline execution status: {execution.describe()['PipelineExecutionStatus']}\")\n",
        "    print(\"\\nStep statuses:\")\n",
        "    for step in execution.list_steps():\n",
        "        print(f\"- {step['StepName']}: {step['StepStatus']}\")\n",
        "\n",
        "# Check status\n",
        "check_pipeline_status(execution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z36X6zEDGLpZ",
        "outputId": "0b0b8293-af6f-43fd-cade-a9bc52de21d8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline execution status: Executing\n",
            "\n",
            "Step statuses:\n",
            "- PreprocessWineData: Executing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6bslg5vLd_gd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain\n",
        "\n",
        "To simulate new data arrival and trigger retraining:"
      ],
      "metadata": {
        "id": "K6KgknBMGOzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_new_data():\n",
        "    \"\"\"Simulate new data arrival and trigger retraining\"\"\"\n",
        "    # Create modified dataset\n",
        "    new_data = wine_data.copy()\n",
        "    new_data['quality'] = new_data['quality'] * 1.1  # Simulate data drift\n",
        "\n",
        "    # Upload to S3 with new timestamp\n",
        "    new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    new_data_path = f\"s3://{bucket}/wine-quality/data/{new_timestamp}/winequality.csv\"\n",
        "    new_data.to_csv(new_data_path, index=False)\n",
        "\n",
        "    # Start new pipeline execution\n",
        "    execution = pipeline.start(\n",
        "        parameters={\n",
        "            'InputDataPath': new_data_path,\n",
        "            'Timestamp': new_timestamp\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return execution\n",
        "\n",
        "# Uncomment to simulate new data and trigger retraining\n",
        "# new_execution = simulate_new_data()"
      ],
      "metadata": {
        "id": "KA7vGOm2GZFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wine Quality ML Pipeline\n",
        "\n",
        "![Wine Quality Pipeline Diagram](https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/Diagrams/WineQualityPipeline.png?raw=1)\n",
        "\n",
        "## Current Pipeline\n",
        "\n",
        "The current ML pipeline for the Wine Quality project consists of four main steps:\n",
        "\n",
        "1. **Preprocessing**: Prepares the raw wine quality data for model training.\n",
        "2. **Training**: Trains the machine learning model using the preprocessed data.\n",
        "3. **Evaluation**: Assesses the performance of the trained model.\n",
        "4. **Condition**: Determines if the model meets the required quality threshold.\n",
        "\n",
        "## Potential Enhancements\n",
        "\n",
        "To further improve the pipeline, consider adding the following steps:\n",
        "\n",
        "1. **Model Registration**: Register the approved model in the SageMaker Model Registry.\n",
        "2. **Deployment**: Automate the deployment of successful models to a SageMaker endpoint.\n",
        "3. **Hyperparameter Tuning**: Implement automated hyperparameter optimization.\n",
        "4. **A/B Testing**: Set up a mechanism for comparing new models against the current production model.\n",
        "5. **Monitoring**: Add continuous monitoring of the deployed model for data drift and performance degradation.\n",
        "6. **Automated Retraining**: Implement a feedback loop to trigger retraining based on monitoring results.\n",
        "7. **Feature Store**: Integrate with SageMaker Feature Store for better feature management.\n",
        "8. **Multi-Model Deployment**: Explore deploying multiple models for different use cases or customer segments.\n",
        "\n",
        "These additions would create a more comprehensive MLOps pipeline, enabling continuous integration, deployment, and improvement of the Wine Quality prediction model."
      ],
      "metadata": {
        "id": "HmCaXp02eSB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "After successfully creating and running the ML pipeline, consider the following advanced steps to enhance your ML workflow:\n",
        "\n",
        "## 1. Model Registry and Version Management\n",
        "- Implement SageMaker Model Registry to catalog and version your models\n",
        "- Set up automated model approval workflows\n",
        "- Create model groups for organizing related models\n",
        "\n",
        "## 2. Inference Optimization\n",
        "- Explore different inference types:\n",
        "  - Real-time inference with SageMaker Endpoints\n",
        "  - Batch transform for large-scale inference\n",
        "  - Serverless Inference for cost-effective, auto-scaling deployments\n",
        "- Implement multi-model endpoints for serving multiple models efficiently\n",
        "\n",
        "## 3. Model Monitoring and Maintenance\n",
        "- Set up data drift detection using SageMaker Model Monitor\n",
        "- Implement automated retraining pipelines based on performance metrics\n",
        "- Establish A/B testing framework for model comparisons\n",
        "\n",
        "## 4. Multi-Region Setup\n",
        "- Deploy models across multiple AWS regions for improved availability and reduced latency\n",
        "- Implement cross-region model replication strategies\n",
        "- Set up global endpoints with Amazon SageMaker Edge Manager\n",
        "\n",
        "## 5. MLOps and CI/CD\n",
        "- Integrate the ML pipeline with CI/CD tools (e.g., AWS CodePipeline, GitHub Actions)\n",
        "- Implement automated testing for data quality, model performance, and infrastructure\n",
        "- Set up approval gates for model deployment to production\n",
        "\n",
        "## 6. Cost Optimization\n",
        "- Analyze and optimize resource utilization (instance types, autoscaling)\n",
        "- Implement SageMaker Savings Plans for long-term cost reduction\n",
        "- Explore SageMaker Managed Spot Training for reduced training costs\n",
        "\n",
        "## 7. Security and Compliance\n",
        "- Implement fine-grained IAM roles and permissions\n",
        "- Set up VPC configurations for enhanced network security\n",
        "- Ensure compliance with relevant regulations (e.g., GDPR, HIPAA) in data handling and model deployment\n",
        "\n",
        "## 8. Advanced Model Techniques\n",
        "- Experiment with ensemble methods and model stacking\n",
        "- Implement automated hyperparameter tuning using SageMaker Hyperparameter Tuning\n",
        "- Explore SageMaker Feature Store for feature management and sharing\n",
        "\n",
        "## 9. Explainability and Fairness\n",
        "- Implement SageMaker Clarify for model explainability and bias detection\n",
        "- Develop custom explainability dashboards for stakeholders\n",
        "- Establish regular audits for model fairness and ethical considerations\n",
        "\n",
        "## 10. Integration with Business Processes\n",
        "- Develop APIs or interfaces for business users to interact with the model\n",
        "- Create visualization dashboards for model performance and business impact\n",
        "- Establish feedback loops for continuous model improvement based on business outcomes\n",
        "\n",
        "By addressing these areas, you'll create a more robust, scalable, and production-ready ML system that can deliver sustained value to your organization."
      ],
      "metadata": {
        "id": "N09QqwNweM6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup (Optional)\n",
        "\n",
        "If you want to clean up resources:\n",
        "1. Stop any running pipeline executions\n",
        "2. Delete the CloudWatch alarms\n",
        "3. Delete the model monitor\n",
        "4. Delete the pipeline\n",
        "\n",
        "Note: Keep these resources if you plan to continue development or monitoring."
      ],
      "metadata": {
        "id": "MIcQsC4dGdKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_resources(pipeline_name=\"WineQualityPipeline\"):\n",
        "    \"\"\"Clean up created resources\"\"\"\n",
        "    try:\n",
        "        # Delete CloudWatch alarm\n",
        "        cloudwatch.delete_alarms(AlarmNames=['WineQualityModelDegradation'])\n",
        "        print(\"CloudWatch alarm deleted\")\n",
        "\n",
        "        # Delete pipeline\n",
        "        sagemaker_client = session.client('sagemaker')\n",
        "        sagemaker_client.delete_pipeline(PipelineName=pipeline_name)\n",
        "        print(\"Pipeline deleted\")\n",
        "\n",
        "        print(\"Cleanup completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cleanup: {str(e)}\")\n",
        "\n",
        "# Uncomment to cleanup resources\n",
        "# cleanup_resources()"
      ],
      "metadata": {
        "id": "V2t8uZOoGfgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}