{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/MLOps/MLPipelineSagemaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Machine Learning Pipeline with Amazon SageMaker\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build an automated ML pipeline using Amazon SageMaker. We'll showcase:\n",
        "- Automated data processing using SageMaker Processing Jobs\n",
        "- Model training using SageMaker Training Jobs\n",
        "- Model deployment using SageMaker Endpoints\n",
        "- Automated monitoring using Model Monitor\n",
        "- Automated retraining using SageMaker Pipelines\n",
        "\n",
        "![ML Pipeline](https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/Diagrams/MLPipeline.png?raw=1)"
      ],
      "metadata": {
        "id": "bpyWlVSZr7Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to implement an AWS SageMaker ML pipeline in Google Colab. Before we proceed with the pipeline implementation, we need to set up AWS credentials.\n",
        "\n",
        "### Credential Setup Method\n",
        "We'll use interactive forms to securely input AWS credentials. This method:\n",
        "- Keeps credentials temporary (only for current session)\n",
        "- Avoids storing sensitive information in the notebook\n",
        "- Uses password fields to hide sensitive input\n",
        "- Clears the form after credentials are set\n",
        "\n",
        "### Required AWS Information\n",
        "You'll need the following information ready:\n",
        "1. **AWS Access Key ID**: Your AWS account access key\n",
        "2. **AWS Secret Access Key**: Your AWS account secret key\n",
        "3. **AWS Region**: The AWS region you want to work in (e.g., 'us-east-1')\n",
        "4. **S3 Bucket**: The name of your S3 bucket for storing pipeline artifacts\n",
        "5. **Role ARN**: The Amazon Resource Name of your IAM role with SageMaker permissions\n",
        "\n",
        "### Prerequisites\n",
        "Make sure you have:\n",
        "- An active AWS account\n",
        "- IAM user with appropriate permissions\n",
        "- S3 bucket created\n",
        "- IAM role configured for SageMaker\n",
        "\n",
        "Run the following cell to set up your credentials:\n"
      ],
      "metadata": {
        "id": "ajz2nIifr-Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 sagemaker ipywidgets s3fs"
      ],
      "metadata": {
        "id": "RC0Z0pqHP5e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting AWS Credentials\n",
        "\n",
        "There are several ways to obtain and use AWS credentials depending on your setup:\n",
        "\n",
        "### If Using Amazon SageMaker Notebook Instance\n",
        "\n",
        "If you're running this notebook in a SageMaker notebook instance, you can leverage the instance's built-in credentials:\n",
        "\n",
        "```python\n",
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "# Get the default SageMaker session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Get the role ARN\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "# Get the default bucket\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "\n",
        "# Get the boto3 session\n",
        "session = boto3.Session()\n",
        "\n",
        "# Print details\n",
        "print(f\"Role ARN: {role}\")\n",
        "print(f\"Default bucket: {default_bucket}\")\n",
        "```"
      ],
      "metadata": {
        "id": "rVx9tUh5u-f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "import sagemaker\n",
        "import logging\n",
        "from IPython.display import clear_output\n",
        "\n",
        "access_key = input(\"AWS Access Key ID: \")\n",
        "secret_key = input(\"AWS Secret Access Key: \")\n",
        "session_token = input(\"AWS Session Token (press Enter if none): \").strip() or None\n",
        "region = input(\"AWS Region (default: us-east-1): \") or \"us-east-1\"\n",
        "bucket = input(\"S3 Bucket Name: \")\n",
        "role_arn = input(\"Role ARN: \")\n",
        "\n",
        "print(\"\\nCredentials set:\")\n",
        "print(f\"Access Key: {access_key[:4]}...{access_key[-4:]}\")\n",
        "print(f\"Secret Key: {secret_key[:4]}...{secret_key[-4:]}\")\n",
        "if session_token:\n",
        "    print(f\"Session Token: {session_token[:4]}...{session_token[-4:]}\")\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"Bucket: {bucket}\")\n",
        "print(f\"Role ARN: {role_arn}\\n\")\n",
        "\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=access_key,\n",
        "    aws_secret_access_key=secret_key,\n",
        "    aws_session_token=session_token,\n",
        "    region_name=region\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "s3 = session.client('s3')\n",
        "bucket_name = bucket\n",
        "try:\n",
        "    s3.head_bucket(Bucket=bucket_name)\n",
        "    print(f\"✓ Successfully accessed S3 bucket: {bucket_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error accessing S3 bucket: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLlyb5wpPlkH",
        "outputId": "5a25338a-00a4-404d-b7e8-fa3e193fc12d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully accessed S3 bucket: yinglonw-test-us-east-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Project Files\n",
        "\n",
        "Before we start building our ML pipeline, we need to get our project files from GitHub. These files contain:\n",
        "\n",
        "- `pipeline.py`: Main pipeline configuration that orchestrates the ML workflow\n",
        "- `scripts/preprocessing.py`: Data preprocessing logic including cleaning and feature engineering\n",
        "- `scripts/training.py`: Model training code using Random Forest algorithm\n",
        "- `scripts/evaluation.py`: Model evaluation metrics calculation\n",
        "\n",
        "The code below will:\n",
        "1. Download these files from GitHub repository\n",
        "2. Create necessary directories in our notebook environment\n",
        "3. Save the files locally so we can use them in our pipeline\n",
        "\n",
        "This setup ensures we have all required scripts available in our SageMaker notebook instance to build and execute our ML pipeline."
      ],
      "metadata": {
        "id": "_Jqdur2YD1Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_github_file(github_url, local_path):\n",
        "    \"\"\"\n",
        "    Download a file from GitHub and save it locally.\n",
        "    Converts GitHub web URL to raw content URL.\n",
        "    \"\"\"\n",
        "    # Convert GitHub URL to raw content URL\n",
        "    raw_url = github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "    # Download and save the file\n",
        "    response = requests.get(raw_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_path, 'w') as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Successfully downloaded: {local_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download: {local_path}\")\n",
        "        print(f\"Status code: {response.status_code}\")\n",
        "\n",
        "# Define the files to download\n",
        "files = {\n",
        "    'pipeline.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/pipeline.py',\n",
        "    'scripts/preprocessing.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/preprocessing.py',\n",
        "    'scripts/training.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/training.py',\n",
        "    'scripts/evaluation.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/evaluation.py'\n",
        "}\n",
        "\n",
        "# Download all files\n",
        "for local_path, github_url in files.items():\n",
        "    download_github_file(github_url, f'wine_quality_pipeline/{local_path}')\n",
        "\n",
        "print(\"\\nChecking downloaded files:\")\n",
        "print(list(Path(\"wine_quality_pipeline\").glob(\"**/*.py\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxQBgMCUD1rF",
        "outputId": "16533727-3367-4e34-c2ea-3b4f3e222bc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded: wine_quality_pipeline/pipeline.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/preprocessing.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/training.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/evaluation.py\n",
            "\n",
            "Checking downloaded files:\n",
            "[PosixPath('wine_quality_pipeline/pipeline.py'), PosixPath('wine_quality_pipeline/scripts/evaluation.py'), PosixPath('wine_quality_pipeline/scripts/preprocessing.py'), PosixPath('wine_quality_pipeline/scripts/training.py')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wine Quality ML Pipeline with Amazon SageMaker\n",
        "\n",
        "This notebook demonstrates how to build an end-to-end machine learning pipeline using Amazon SageMaker. We'll use the Wine Quality dataset to showcase:\n",
        "- Data preprocessing\n",
        "- Model training\n",
        "- Model evaluation\n",
        "- Automated retraining\n",
        "- Model monitoring\n",
        "\n",
        "The pipeline will automatically handle data preprocessing, model training, and evaluation, making it easy to retrain models when new data arrives."
      ],
      "metadata": {
        "id": "PrTTPpNCBwac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Import Required Libraries\n",
        "\n",
        "First, let's import our required libraries and setup our project structure."
      ],
      "metadata": {
        "id": "blY-Gi6bDUMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our pipeline creation function\n",
        "from wine_quality_pipeline.pipeline import create_pipeline\n",
        "\n",
        "print(\"Current working directory:\", Path.cwd())\n",
        "print(\"\\nContents of wine_quality_pipeline:\")\n",
        "print(list(Path(\"wine_quality_pipeline\").glob(\"**/*.py\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZXt0YxsDWDA",
        "outputId": "92800f99-12dc-46aa-be04-36a816387d2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "\n",
            "Contents of wine_quality_pipeline:\n",
            "[PosixPath('wine_quality_pipeline/pipeline.py'), PosixPath('wine_quality_pipeline/scripts/evaluation.py'), PosixPath('wine_quality_pipeline/scripts/preprocessing.py'), PosixPath('wine_quality_pipeline/scripts/training.py')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Prepare Initial Dataset\n",
        "\n",
        "First, let's download the Wine Quality dataset and upload it to our S3 bucket. We'll use this as our initial training data."
      ],
      "metadata": {
        "id": "gxEPMErNBzD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import io\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Download wine quality dataset\n",
        "wine_data = pd.read_csv(\n",
        "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
        "    sep=';'\n",
        ")\n",
        "\n",
        "# Create a timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Define S3 path\n",
        "initial_data_path = f\"s3://{bucket}/wine-quality/data/{timestamp}/winequality.csv\"\n",
        "\n",
        "# Convert DataFrame to CSV buffer\n",
        "csv_buffer = io.StringIO()\n",
        "wine_data.to_csv(csv_buffer, index=False)\n",
        "\n",
        "# Parse S3 URL to get bucket and key\n",
        "def parse_s3_url(s3_url):\n",
        "    parsed = urlparse(s3_url)\n",
        "    bucket = parsed.netloc\n",
        "    key = parsed.path.lstrip('/')\n",
        "    return bucket, key\n",
        "\n",
        "bucket_name, key_path = parse_s3_url(initial_data_path)\n",
        "\n",
        "# Upload using the session's S3 client\n",
        "s3 = session.client('s3')\n",
        "s3.put_object(\n",
        "    Bucket=bucket_name,\n",
        "    Key=key_path,\n",
        "    Body=csv_buffer.getvalue()\n",
        ")\n",
        "\n",
        "print(f\"Data uploaded to: {initial_data_path}\")\n",
        "print(f\"Dataset shape: {wine_data.shape}\")\n",
        "print(\"\\nFeatures:\")\n",
        "print(wine_data.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OjxsB57Bw81",
        "outputId": "c87e9697-5206-4be7-bf6f-7b148e3a1f51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data uploaded to: s3://yinglonw-test-us-east-1/wine-quality/data/20250216_184323/winequality.csv\n",
            "Dataset shape: (1599, 12)\n",
            "\n",
            "Features:\n",
            "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Scripts to S3\n",
        "\n",
        "Now, let's upload our existing preprocessing, training, and evaluation scripts to S3."
      ],
      "metadata": {
        "id": "bs94jGAYExgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_scripts_to_s3(boto3_session, bucket, prefix=\"wine-quality/code\"):\n",
        "    # Get S3 client from boto3 session\n",
        "    s3_client = boto3_session.client('s3')\n",
        "\n",
        "    # Define the scripts to upload\n",
        "    scripts = {\n",
        "        'preprocessing.py': 'wine_quality_pipeline/scripts/preprocessing.py',\n",
        "        'training.py': 'wine_quality_pipeline/scripts/training.py',\n",
        "        'evaluation.py': 'wine_quality_pipeline/scripts/evaluation.py'\n",
        "    }\n",
        "\n",
        "    # Upload each script\n",
        "    for s3_key, local_path in scripts.items():\n",
        "        try:\n",
        "            s3_client.upload_file(\n",
        "                local_path,\n",
        "                bucket,\n",
        "                f\"{prefix}/{s3_key}\"\n",
        "            )\n",
        "            print(f\"Uploaded {local_path} to s3://{bucket}/{prefix}/{s3_key}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
        "\n",
        "    return f\"s3://{bucket}/{prefix}\"\n",
        "\n",
        "# Upload scripts using the boto3 session\n",
        "script_prefix = upload_scripts_to_s3(session, bucket)\n",
        "print(\"\\nAll scripts uploaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8pq4wvIB21s",
        "outputId": "d5e0c70a-d37c-4266-f7a0-b327844e9580"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded wine_quality_pipeline/scripts/preprocessing.py to s3://yinglonw-test-us-east-1/wine-quality/code/preprocessing.py\n",
            "Uploaded wine_quality_pipeline/scripts/training.py to s3://yinglonw-test-us-east-1/wine-quality/code/training.py\n",
            "Uploaded wine_quality_pipeline/scripts/evaluation.py to s3://yinglonw-test-us-east-1/wine-quality/code/evaluation.py\n",
            "\n",
            "All scripts uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Configure Pipeline\n",
        "\n",
        "Now we'll create our SageMaker pipeline using our existing pipeline configuration."
      ],
      "metadata": {
        "id": "WRSt4u9RFXxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipeline, model_monitor = create_pipeline(\n",
        "    role=role_arn,\n",
        "    bucket=bucket,\n",
        "    pipeline_name=\"WineQualityPipeline\",\n",
        "    base_job_prefix=\"wine-quality\"\n",
        ")\n",
        "\n",
        "print(\"Pipeline created successfully!\")"
      ],
      "metadata": {
        "id": "DjjGRLcjEzsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Model Monitoring\n",
        "\n",
        "Configure monitoring for our model to track its performance over time."
      ],
      "metadata": {
        "id": "5lmprYLVGCsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_cloudwatch_alerts(cloudwatch_client):\n",
        "    \"\"\"Setup CloudWatch alerts for model monitoring\"\"\"\n",
        "    try:\n",
        "        # Example alert for model performance degradation\n",
        "        cloudwatch_client.put_metric_alarm(\n",
        "            AlarmName='WineQualityModelDegradation',\n",
        "            MetricName='mse',\n",
        "            Namespace='WineQualityModel',\n",
        "            Statistic='Average',\n",
        "            Period=300,\n",
        "            EvaluationPeriods=2,\n",
        "            Threshold=0.5,\n",
        "            ComparisonOperator='GreaterThanThreshold',\n",
        "            AlarmActions=[role_arn]  # Replace with your SNS topic if needed\n",
        "        )\n",
        "        print(\"CloudWatch alert created successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating CloudWatch alert: {str(e)}\")\n",
        "\n",
        "# Setup CloudWatch alerts\n",
        "cloudwatch = session.client('cloudwatch')\n",
        "setup_cloudwatch_alerts(cloudwatch)\n",
        "\n",
        "print(\"Model monitoring setup complete!\")"
      ],
      "metadata": {
        "id": "JuGMKzPFFagI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Pipeline\n",
        "\n",
        "Finally, let's execute our pipeline and start monitoring."
      ],
      "metadata": {
        "id": "-VILXqdPGGNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the pipeline\n",
        "pipeline.upsert(role_arn=role_arn)\n",
        "execution = pipeline.start()\n",
        "\n",
        "print(f\"Pipeline execution started with ARN: {execution.arn}\")"
      ],
      "metadata": {
        "id": "EvTRXF7ZGIB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monitor Pipeline Execution\n",
        "\n",
        "Let's check the status of our pipeline execution."
      ],
      "metadata": {
        "id": "GBUaJiGnGJxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pipeline_status(execution):\n",
        "    \"\"\"Monitor the pipeline execution status\"\"\"\n",
        "    print(f\"Pipeline execution status: {execution.describe()['PipelineExecutionStatus']}\")\n",
        "    print(\"\\nStep statuses:\")\n",
        "    for step in execution.list_steps():\n",
        "        print(f\"- {step['StepName']}: {step['StepStatus']}\")\n",
        "\n",
        "# Check status\n",
        "check_pipeline_status(execution)"
      ],
      "metadata": {
        "id": "z36X6zEDGLpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "You can now:\n",
        "1. Monitor the pipeline execution in the SageMaker console\n",
        "2. Check CloudWatch metrics for model performance\n",
        "3. Set up automated retraining when performance degrades\n",
        "4. Add new data to trigger model retraining\n",
        "\n",
        "To simulate new data arrival and trigger retraining:"
      ],
      "metadata": {
        "id": "K6KgknBMGOzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_new_data():\n",
        "    \"\"\"Simulate new data arrival and trigger retraining\"\"\"\n",
        "    # Create modified dataset\n",
        "    new_data = wine_data.copy()\n",
        "    new_data['quality'] = new_data['quality'] * 1.1  # Simulate data drift\n",
        "\n",
        "    # Upload to S3 with new timestamp\n",
        "    new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    new_data_path = f\"s3://{bucket}/wine-quality/data/{new_timestamp}/winequality.csv\"\n",
        "    new_data.to_csv(new_data_path, index=False)\n",
        "\n",
        "    # Start new pipeline execution\n",
        "    execution = pipeline.start(\n",
        "        parameters={\n",
        "            'InputDataPath': new_data_path,\n",
        "            'Timestamp': new_timestamp\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return execution\n",
        "\n",
        "# Uncomment to simulate new data and trigger retraining\n",
        "# new_execution = simulate_new_data()"
      ],
      "metadata": {
        "id": "KA7vGOm2GZFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup (Optional)\n",
        "\n",
        "If you want to clean up resources:\n",
        "1. Stop any running pipeline executions\n",
        "2. Delete the CloudWatch alarms\n",
        "3. Delete the model monitor\n",
        "4. Delete the pipeline\n",
        "\n",
        "Note: Keep these resources if you plan to continue development or monitoring."
      ],
      "metadata": {
        "id": "MIcQsC4dGdKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup_resources(pipeline_name=\"WineQualityPipeline\"):\n",
        "    \"\"\"Clean up created resources\"\"\"\n",
        "    try:\n",
        "        # Delete CloudWatch alarm\n",
        "        cloudwatch.delete_alarms(AlarmNames=['WineQualityModelDegradation'])\n",
        "        print(\"CloudWatch alarm deleted\")\n",
        "\n",
        "        # Delete pipeline\n",
        "        sagemaker_client = session.client('sagemaker')\n",
        "        sagemaker_client.delete_pipeline(PipelineName=pipeline_name)\n",
        "        print(\"Pipeline deleted\")\n",
        "\n",
        "        print(\"Cleanup completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during cleanup: {str(e)}\")\n",
        "\n",
        "# Uncomment to cleanup resources\n",
        "# cleanup_resources()"
      ],
      "metadata": {
        "id": "V2t8uZOoGfgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}