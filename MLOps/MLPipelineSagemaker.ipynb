{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/MLOps/MLPipelineSagemaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Machine Learning Pipeline with Amazon SageMaker\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to build an automated ML pipeline using Amazon SageMaker. We'll showcase:\n",
        "- Automated data processing using SageMaker Processing Jobs\n",
        "- Model training using SageMaker Training Jobs\n",
        "- Model deployment using SageMaker Endpoints\n",
        "- Automated monitoring using Model Monitor\n",
        "- Automated retraining using SageMaker Pipelines\n",
        "\n",
        "![ML Pipeline](https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/Diagrams/MLPipeline.png?raw=1)"
      ],
      "metadata": {
        "id": "bpyWlVSZr7Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to implement an AWS SageMaker ML pipeline in Google Colab. Before we proceed with the pipeline implementation, we need to set up AWS credentials.\n",
        "\n",
        "### Credential Setup Method\n",
        "We'll use interactive forms to securely input AWS credentials. This method:\n",
        "- Keeps credentials temporary (only for current session)\n",
        "- Avoids storing sensitive information in the notebook\n",
        "- Uses password fields to hide sensitive input\n",
        "- Clears the form after credentials are set\n",
        "\n",
        "### Required AWS Information\n",
        "You'll need the following information ready:\n",
        "1. **AWS Access Key ID**: Your AWS account access key\n",
        "2. **AWS Secret Access Key**: Your AWS account secret key\n",
        "3. **AWS Region**: The AWS region you want to work in (e.g., 'us-east-1')\n",
        "4. **S3 Bucket**: The name of your S3 bucket for storing pipeline artifacts\n",
        "5. **Role ARN**: The Amazon Resource Name of your IAM role with SageMaker permissions\n",
        "\n",
        "### Prerequisites\n",
        "Make sure you have:\n",
        "- An active AWS account\n",
        "- IAM user with appropriate permissions\n",
        "- S3 bucket created\n",
        "- IAM role configured for SageMaker\n",
        "\n",
        "Run the following cell to set up your credentials:\n"
      ],
      "metadata": {
        "id": "ajz2nIifr-Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 sagemaker ipywidgets s3fs"
      ],
      "metadata": {
        "id": "RC0Z0pqHP5e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting AWS Credentials\n",
        "\n",
        "There are several ways to obtain and use AWS credentials depending on your setup:\n",
        "\n",
        "### If Using Amazon SageMaker Notebook Instance\n",
        "\n",
        "If you're running this notebook in a SageMaker notebook instance, you can leverage the instance's built-in credentials:\n",
        "\n",
        "```python\n",
        "import sagemaker\n",
        "import boto3\n",
        "\n",
        "# Get the default SageMaker session\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "# Get the role ARN\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "# Get the default bucket\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "\n",
        "# Get the boto3 session\n",
        "session = boto3.Session()\n",
        "\n",
        "# Print details\n",
        "print(f\"Role ARN: {role}\")\n",
        "print(f\"Default bucket: {default_bucket}\")\n",
        "```"
      ],
      "metadata": {
        "id": "rVx9tUh5u-f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "import sagemaker\n",
        "import logging\n",
        "from IPython.display import clear_output\n",
        "\n",
        "access_key = input(\"AWS Access Key ID: \")\n",
        "secret_key = input(\"AWS Secret Access Key: \")\n",
        "session_token = input(\"AWS Session Token (press Enter if none): \").strip() or None\n",
        "region = input(\"AWS Region (default: us-east-1): \") or \"us-east-1\"\n",
        "bucket = input(\"S3 Bucket Name: \")\n",
        "role_arn = input(\"Role ARN: \")\n",
        "\n",
        "print(\"\\nCredentials set:\")\n",
        "print(f\"Access Key: {access_key[:4]}...{access_key[-4:]}\")\n",
        "print(f\"Secret Key: {secret_key[:4]}...{secret_key[-4:]}\")\n",
        "if session_token:\n",
        "    print(f\"Session Token: {session_token[:4]}...{session_token[-4:]}\")\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"Bucket: {bucket}\")\n",
        "print(f\"Role ARN: {role_arn}\\n\")\n",
        "\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id=access_key,\n",
        "    aws_secret_access_key=secret_key,\n",
        "    aws_session_token=session_token,\n",
        "    region_name=region\n",
        ")\n",
        "\n",
        "clear_output()\n",
        "\n",
        "s3 = session.client('s3')\n",
        "bucket_name = bucket\n",
        "try:\n",
        "    s3.head_bucket(Bucket=bucket_name)\n",
        "    print(f\"✓ Successfully accessed S3 bucket: {bucket_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error accessing S3 bucket: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLlyb5wpPlkH",
        "outputId": "5a25338a-00a4-404d-b7e8-fa3e193fc12d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully accessed S3 bucket: yinglonw-test-us-east-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Project Files\n",
        "\n",
        "Before we start building our ML pipeline, we need to get our project files from GitHub. These files contain:\n",
        "\n",
        "- `pipeline.py`: Main pipeline configuration that orchestrates the ML workflow\n",
        "- `scripts/preprocessing.py`: Data preprocessing logic including cleaning and feature engineering\n",
        "- `scripts/training.py`: Model training code using Random Forest algorithm\n",
        "- `scripts/evaluation.py`: Model evaluation metrics calculation\n",
        "\n",
        "The code below will:\n",
        "1. Download these files from GitHub repository\n",
        "2. Create necessary directories in our notebook environment\n",
        "3. Save the files locally so we can use them in our pipeline\n",
        "\n",
        "This setup ensures we have all required scripts available in our SageMaker notebook instance to build and execute our ML pipeline."
      ],
      "metadata": {
        "id": "_Jqdur2YD1Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_github_file(github_url, local_path):\n",
        "    \"\"\"\n",
        "    Download a file from GitHub and save it locally.\n",
        "    Converts GitHub web URL to raw content URL.\n",
        "    \"\"\"\n",
        "    # Convert GitHub URL to raw content URL\n",
        "    raw_url = github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "    # Download and save the file\n",
        "    response = requests.get(raw_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_path, 'w') as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Successfully downloaded: {local_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download: {local_path}\")\n",
        "        print(f\"Status code: {response.status_code}\")\n",
        "\n",
        "# Define the files to download\n",
        "files = {\n",
        "    'pipeline.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/pipeline.py',\n",
        "    'scripts/preprocessing.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/preprocessing.py',\n",
        "    'scripts/training.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/training.py',\n",
        "    'scripts/evaluation.py': 'https://github.com/calmrocks/master-machine-learning-engineer/blob/main/MLOps/wine_quality_pipeline/scripts/evaluation.py'\n",
        "}\n",
        "\n",
        "# Download all files\n",
        "for local_path, github_url in files.items():\n",
        "    download_github_file(github_url, f'wine_quality_pipeline/{local_path}')\n",
        "\n",
        "print(\"\\nChecking downloaded files:\")\n",
        "print(list(Path(\"wine_quality_pipeline\").glob(\"**/*.py\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxQBgMCUD1rF",
        "outputId": "16533727-3367-4e34-c2ea-3b4f3e222bc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded: wine_quality_pipeline/pipeline.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/preprocessing.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/training.py\n",
            "Successfully downloaded: wine_quality_pipeline/scripts/evaluation.py\n",
            "\n",
            "Checking downloaded files:\n",
            "[PosixPath('wine_quality_pipeline/pipeline.py'), PosixPath('wine_quality_pipeline/scripts/evaluation.py'), PosixPath('wine_quality_pipeline/scripts/preprocessing.py'), PosixPath('wine_quality_pipeline/scripts/training.py')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wine Quality ML Pipeline with Amazon SageMaker\n",
        "\n",
        "This notebook demonstrates how to build an end-to-end machine learning pipeline using Amazon SageMaker. We'll use the Wine Quality dataset to showcase:\n",
        "- Data preprocessing\n",
        "- Model training\n",
        "- Model evaluation\n",
        "- Automated retraining\n",
        "- Model monitoring\n",
        "\n",
        "The pipeline will automatically handle data preprocessing, model training, and evaluation, making it easy to retrain models when new data arrives."
      ],
      "metadata": {
        "id": "PrTTPpNCBwac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Import Required Libraries\n",
        "\n",
        "First, let's import our required libraries and setup our project structure."
      ],
      "metadata": {
        "id": "blY-Gi6bDUMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our pipeline creation function\n",
        "from wine_quality_pipeline.pipeline import create_pipeline\n",
        "\n",
        "print(\"Current working directory:\", Path.cwd())\n",
        "print(\"\\nContents of wine_quality_pipeline:\")\n",
        "print(list(Path(\"wine_quality_pipeline\").glob(\"**/*.py\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZXt0YxsDWDA",
        "outputId": "92800f99-12dc-46aa-be04-36a816387d2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "\n",
            "Contents of wine_quality_pipeline:\n",
            "[PosixPath('wine_quality_pipeline/pipeline.py'), PosixPath('wine_quality_pipeline/scripts/evaluation.py'), PosixPath('wine_quality_pipeline/scripts/preprocessing.py'), PosixPath('wine_quality_pipeline/scripts/training.py')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Prepare Initial Dataset\n",
        "\n",
        "First, let's download the Wine Quality dataset and upload it to our S3 bucket. We'll use this as our initial training data."
      ],
      "metadata": {
        "id": "gxEPMErNBzD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import io\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Download wine quality dataset\n",
        "wine_data = pd.read_csv(\n",
        "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
        "    sep=';'\n",
        ")\n",
        "\n",
        "# Create a timestamp for versioning\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Define S3 path\n",
        "initial_data_path = f\"s3://{bucket}/wine-quality/data/{timestamp}/winequality.csv\"\n",
        "\n",
        "# Convert DataFrame to CSV buffer\n",
        "csv_buffer = io.StringIO()\n",
        "wine_data.to_csv(csv_buffer, index=False)\n",
        "\n",
        "# Parse S3 URL to get bucket and key\n",
        "def parse_s3_url(s3_url):\n",
        "    parsed = urlparse(s3_url)\n",
        "    bucket = parsed.netloc\n",
        "    key = parsed.path.lstrip('/')\n",
        "    return bucket, key\n",
        "\n",
        "bucket_name, key_path = parse_s3_url(initial_data_path)\n",
        "\n",
        "# Upload using the session's S3 client\n",
        "s3 = session.client('s3')\n",
        "s3.put_object(\n",
        "    Bucket=bucket_name,\n",
        "    Key=key_path,\n",
        "    Body=csv_buffer.getvalue()\n",
        ")\n",
        "\n",
        "print(f\"Data uploaded to: {initial_data_path}\")\n",
        "print(f\"Dataset shape: {wine_data.shape}\")\n",
        "print(\"\\nFeatures:\")\n",
        "print(wine_data.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OjxsB57Bw81",
        "outputId": "c87e9697-5206-4be7-bf6f-7b148e3a1f51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data uploaded to: s3://yinglonw-test-us-east-1/wine-quality/data/20250216_184323/winequality.csv\n",
            "Dataset shape: (1599, 12)\n",
            "\n",
            "Features:\n",
            "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8pq4wvIB21s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}