{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/BasicModels/DeepLearningTrainingProcess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Deep Learning Training Process\n",
        "\n",
        "## Introduction\n",
        "This notebook demonstrates the training process in deep learning, focusing on:\n",
        "1. Forward Propagation\n",
        "2. Loss Calculation\n",
        "3. Backward Propagation\n",
        "4. Parameter Updates\n",
        "\n",
        "We'll implement a simple neural network from scratch to understand these concepts better."
      ],
      "metadata": {
        "id": "ySYEKTxxTILl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EfB6wyCSbxt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Creating a Simple Neural Network Class\n",
        "\n",
        "First, let's create a simple neural network with one hidden layer to understand the training process:class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        # Store parameters and gradients\n",
        "        self.parameters = {'W1': self.W1, 'b1': self.b1, 'W2': self.W2, 'b2': self.b2}\n",
        "        self.gradients = {}\n",
        "        self.cache = {}\n",
        "        \n",
        "    def sigmoid(self, Z: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "    \n",
        "    def sigmoid_derivative(self, Z: np.ndarray) -> np.ndarray:\n",
        "        s = self.sigmoid(Z)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def forward_propagation(self, X: np.ndarray) -> np.ndarray:\n",
        "        # First layer\n",
        "        Z1 = np.dot(X, self.W1) + self.b1\n",
        "        A1 = self.sigmoid(Z1)\n",
        "        \n",
        "        # Second layer\n",
        "        Z2 = np.dot(A1, self.W2) + self.b2\n",
        "        A2 = self.sigmoid(Z2)\n",
        "        \n",
        "        # Store values for backpropagation\n",
        "        self.cache = {\n",
        "            'Z1': Z1, 'A1': A1,\n",
        "            'Z2': Z2, 'A2': A2,\n",
        "            'X': X\n",
        "        }\n",
        "        \n",
        "        return A2\n",
        "    \n",
        "    def compute_loss(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
        "        m = Y.shape[0]\n",
        "        loss = -1/m * np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))\n",
        "        return loss\n",
        "    \n",
        "    def backward_propagation(self, Y: np.ndarray) -> None:\n",
        "        m = Y.shape[0]\n",
        "        \n",
        "        # Output layer\n",
        "        dZ2 = self.cache['A2'] - Y\n",
        "        dW2 = 1/m * np.dot(self.cache['A1'].T, dZ2)\n",
        "        db2 = 1/m * np.sum(dZ2, axis=0, keepdims=True)\n",
        "        \n",
        "        # Hidden layer\n",
        "        dZ1 = np.dot(dZ2, self.W2.T) * self.sigmoid_derivative(self.cache['Z1'])\n",
        "        dW1 = 1/m * np.dot(self.cache['X'].T, dZ1)\n",
        "        db1 = 1/m * np.sum(dZ1, axis=0, keepdims=True)\n",
        "        \n",
        "        # Store gradients\n",
        "        self.gradients = {\n",
        "            'dW1': dW1, 'db1': db1,\n",
        "            'dW2': dW2, 'db2': db2\n",
        "        }\n",
        "    \n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.W1 -= learning_rate * self.gradients['dW1']\n",
        "        self.b1 -= learning_rate * self.gradients['db1']\n",
        "        self.W2 -= learning_rate * self.gradients['dW2']\n",
        "        self.b2 -= learning_rate * self.gradients['db2']\n",
        "        \n",
        "        # Update stored parameters\n",
        "        self.parameters = {'W1': self.W1, 'b1': self.b1, 'W2': self.W2, 'b2': self.b2}"
      ],
      "metadata": {
        "id": "zdEIhyoJT8di"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vnpq4AjJT7nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYFw8iR4T5PY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}