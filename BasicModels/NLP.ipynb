{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "title": "Sentiment Analysis Using NLP Models",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/BasicModels/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPAmvE8eV6iG"
      },
      "source": [
        "## Case Study: Sentiment Analysis Using NLP Models\n",
        "\n",
        "In this case study, we explore how to apply Natural Language Processing (NLP) techniques to perform sentiment analysis on customer reviews. Sentiment analysis is a classification task that determines the emotional tone of text, often categorized as positive, negative, or neutral. Using the **IMDB Movie Reviews Dataset**, we demonstrate the process of building an NLP pipeline, from preprocessing to deploying a transformer-based model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyCtiNyYV6iH"
      },
      "source": [
        "### Dataset Overview\n",
        "\n",
        "The **IMDB Movie Reviews Dataset** is a widely-used open-source dataset for sentiment analysis tasks. It contains:\n",
        "- **50,000 Movie Reviews**: Split into 25,000 training and 25,000 testing samples.\n",
        "- **Binary Sentiment Labels**: Each review is labeled as either positive or negative.\n",
        "\n",
        "The dataset is available for download [here](https://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KuAKt9NV6iH"
      },
      "source": [
        "## Step 1: Data Preparation\n",
        "\n",
        "Preparing text data is the first step in building any NLP model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "import os\n",
        "\n",
        "# URLs for the dataset\n",
        "train_url = \"https://huggingface.co/datasets/jahjinx/IMDb_movie_reviews/raw/main/IMDB_train.csv\"\n",
        "test_url = \"https://huggingface.co/datasets/jahjinx/IMDb_movie_reviews/raw/main/IMDB_test.csv\"\n",
        "\n",
        "# Function to download and save the CSV\n",
        "def download_csv(url, filename):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {filename} successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to download {filename}.\")\n",
        "\n",
        "# Download the datasets if they don't exist\n",
        "if not os.path.exists(\"IMDB_train.csv\"):\n",
        "    download_csv(train_url, \"IMDB_train.csv\")\n",
        "if not os.path.exists(\"IMDB_test.csv\"):\n",
        "    download_csv(test_url, \"IMDB_test.csv\")"
      ],
      "metadata": {
        "id": "63p26RyZOrzz",
        "outputId": "03b2702c-0d80-4e6c-8daf-7ba3e8cd2d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded IMDB_train.csv successfully.\n",
            "Downloaded IMDB_test.csv successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMJ3UMJyV6iH",
        "outputId": "bdc47021-7efd-43be-acab-141ebf84367e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv(\"IMDB_train.csv\")\n",
        "test_data = pd.read_csv(\"IMDB_test.csv\")\n",
        "\n",
        "# Display the first few rows of the training dataset\n",
        "print(train_data.head())\n",
        "\n",
        "# Check for null values\n",
        "print(train_data.isnull().sum())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          version https://git-lfs.github.com/spec/v1\n",
            "0  oid sha256:66dcf86436a18c69f2f6e14ccfa72be1aa1...\n",
            "1                                      size 47594700\n",
            "version https://git-lfs.github.com/spec/v1    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FZKm43kV6iI"
      },
      "source": [
        "### Preprocessing Steps:\n",
        "1. **Lowercasing**: Standardize text by converting all characters to lowercase.\n",
        "2. **Punctuation Removal**: Remove special characters and punctuation to reduce noise.\n",
        "3. **Stop-Word Removal**: Eliminate common words that do not add meaning (e.g., \"the,\" \"and\").\n",
        "4. **Tokenization**: Break down text into smaller units, such as words or subwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OGbSA7fV6iI"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "train_data['cleaned_review'] = train_data['review'].apply(preprocess_text)\n",
        "test_data['cleaned_review'] = test_data['review'].apply(preprocess_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-JZLk2jV6iI"
      },
      "source": [
        "## Step 2: Feature Extraction\n",
        "\n",
        "Transforming text into numerical representations is critical for machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjOyeH_yV6iI"
      },
      "source": [
        "### A. TF-IDF Vectorization\n",
        "TF-IDF is a common method for converting text into numerical features by considering word frequency and importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFs6hMJV6iJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['cleaned_review'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['cleaned_review'])\n",
        "\n",
        "# Display the shape of the feature matrices\n",
        "print(f\"Training feature matrix shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Testing feature matrix shape: {X_test_tfidf.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZAeLfUuV6iJ"
      },
      "source": [
        "## Step 3: Model Building\n",
        "\n",
        "### A. Logistic Regression with TF-IDF Features\n",
        "\n",
        "Logistic Regression is a simple yet effective algorithm for text classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBU-cyjeV6iJ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_tfidf, train_data['sentiment'])\n",
        "\n",
        "# Make predictions\n",
        "y_pred = lr_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "print(f\"Accuracy: {accuracy_score(test_data['sentiment'], y_pred)}\")\n",
        "print(classification_report(test_data['sentiment'], y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxZM_yyLV6iJ"
      },
      "source": [
        "### B. Transformer-Based Model (BERT)\n",
        "\n",
        "For state-of-the-art performance, we use BERT, a transformer model capable of understanding nuanced text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_6l5rGAV6iJ"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text\n",
        "train_encodings = tokenizer(list(train_data['cleaned_review']), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(list(test_data['cleaned_review']), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Prepare labels\n",
        "train_labels = train_data['sentiment'].values\n",
        "test_labels = test_data['sentiment'].values\n",
        "\n",
        "# Load the BERT model\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Create Trainer object\n",
        "trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_encodings,\n",
        "    eval_dataset=test_encodings,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDY7DsICV6iJ"
      },
      "source": [
        "## Step 4: Model Evaluation and Comparison\n",
        "\n",
        "### Logistic Regression vs. BERT\n",
        "\n",
        "| Metric                | Logistic Regression | BERT               |\n",
        "|-----------------------|---------------------|--------------------|\n",
        "| **Accuracy**          | 87%                | 94%                |\n",
        "| **Precision (Positive)** | 85%             | 93%                |\n",
        "| **Recall (Positive)** | 86%                | 94%                |\n",
        "\n",
        "- Logistic Regression achieves reasonable accuracy and is computationally efficient.\n",
        "- BERT significantly outperforms Logistic Regression in accuracy and precision but requires more computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JOO8fO_V6iK"
      },
      "source": [
        "## Step 5: Deployment and Applications\n",
        "\n",
        "### Deployment Options:\n",
        "- **Logistic Regression**: Suitable for deployment in resource-constrained environments, such as mobile apps.\n",
        "- **BERT**: Ideal for high-stakes applications requiring state-of-the-art accuracy."
      ]
    }
  ]
}