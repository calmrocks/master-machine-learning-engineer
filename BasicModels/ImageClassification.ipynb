{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "title": "Building an Image Classification Model with Transfer Learning"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Case Study: Building an Image Classification Model with Transfer Learning\n\nThis case study demonstrates how to develop an image classification model using transfer learning on the **CIFAR-10 Dataset**. We explore preprocessing techniques, feature extraction, and transfer learning with a pretrained model to classify images into ten categories."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Dataset Overview\n\nThe **CIFAR-10 Dataset** is a widely used benchmark for image classification tasks. It contains:\n- **Images**: 60,000 color images, each 32x32 pixels.\n- **Classes**: 10 categories such as airplanes, automobiles, birds, cats, and dogs.\n- **Split**: 50,000 training images and 10,000 test images.\n\nThe dataset is available through the [CIFAR-10 website](https://www.cs.toronto.edu/~kriz/cifar.html)."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 1: Data Preparation"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nimport matplotlib.pyplot as plt\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Display a few sample images\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(x_train[i])\n    plt.title(class_names[y_train[i][0]])\n    plt.axis('off')\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Preprocessing Steps:\n1. **Normalization**: Scale pixel values to the range [0, 1] to stabilize training.\n2. **One-Hot Encoding**: Convert class labels into one-hot vectors for compatibility with classification models."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Normalize pixel values\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n# One-hot encode labels\ny_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\ny_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 2: Data Augmentation"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define augmentation transformations\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    horizontal_flip=True,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    zoom_range=0.2\n)\n\n# Apply augmentation\ndatagen.fit(x_train)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 3: Transfer Learning with Pretrained Models\n\nUsing a pretrained model such as ResNet50 allows leveraging learned features from large-scale datasets like ImageNet, reducing training time and improving performance."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# Load the pretrained ResNet50 model without the top layer\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\n# Freeze the base model layers\nbase_model.trainable = False\n\n# Add custom classification layers\nx = Flatten()(base_model.output)\nx = Dense(128, activation='relu')(x)\noutput = Dense(10, activation='softmax')(x)\n\n# Create the final model\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(datagen.flow(x_train, y_train, batch_size=32), epochs=10, validation_data=(x_test, y_test))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 4: Model Evaluation and Optimization"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Plot training and validation accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 5: Deployment and Applications\n\n### Deployment Options:\n- **Batch Inference**: Use the model for batch predictions on datasets, such as categorizing large image collections.\n- **Real-Time Classification**: Deploy the model on a web application or mobile app for live image recognition.\n\n### Applications:\n1. **E-Commerce**: Automatically categorize product images for streamlined inventory management.\n2. **Healthcare**: Classify medical images to aid diagnostics, such as detecting diseases in X-rays.\n3. **Autonomous Vehicles**: Recognize objects like pedestrians and traffic signs in real-time."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Summary and Recommendations\n\n### Summary:\n- **Preprocessing**: Normalization and augmentation improve model robustness and performance.\n- **Transfer Learning**: Pretrained models like ResNet50 reduce training time and achieve higher accuracy with smaller datasets.\n- **Evaluation and Optimization**: Monitoring training and validation performance ensures the model generalizes well.\n\n### Recommendations:\n- Use **data augmentation** to artificially expand training datasets, especially when data is limited.\n- Apply **transfer learning** to leverage powerful pretrained models for small or medium-sized datasets.\n- Continuously monitor performance metrics during training to detect overfitting or underfitting.\n\nBy following these steps, practitioners can build robust image classification models that generalize well to real-world applications, making them suitable for deployment in diverse domains."
    }
  ]
}
