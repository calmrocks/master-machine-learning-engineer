{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d8ae2f30b9d4b46a0878413dbca1fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fe6165195e649768f9a073fa3d29bc3",
              "IPY_MODEL_859fbbb46a734f4da99c3e12c052322f",
              "IPY_MODEL_f028385da6764b5f94e525bf29b38a4f"
            ],
            "layout": "IPY_MODEL_ba378fa0d2564d2199efc73723aaeeb3"
          }
        },
        "5fe6165195e649768f9a073fa3d29bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f767e29ecc34829936ff2d590054d2f",
            "placeholder": "​",
            "style": "IPY_MODEL_68798ad0d08e4fecb46e7f8d15aedf94",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "859fbbb46a734f4da99c3e12c052322f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b145075f724e02ba8def5ac38f0085",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7a5b867feb64e80be28f7c5650cda79",
            "value": 2
          }
        },
        "f028385da6764b5f94e525bf29b38a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc9f1506acfe42b2b668d11f63b25674",
            "placeholder": "​",
            "style": "IPY_MODEL_72811a747a6c458aa27cc1c4c57f6213",
            "value": " 2/2 [00:01&lt;00:00,  1.98it/s]"
          }
        },
        "ba378fa0d2564d2199efc73723aaeeb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f767e29ecc34829936ff2d590054d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68798ad0d08e4fecb46e7f8d15aedf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16b145075f724e02ba8def5ac38f0085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7a5b867feb64e80be28f7c5650cda79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc9f1506acfe42b2b668d11f63b25674": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72811a747a6c458aa27cc1c4c57f6213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/modules/generative-ai/materials/notebooks/llm_benchmark_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Foundation Models Comparison\n",
        "\n",
        "This notebook compares different LLM foundation models across various aspects including:\n",
        "- Response quality\n",
        "- Performance metrics\n",
        "- Resource usage\n",
        "- Practical considerations\n",
        "\n",
        "We'll test models from different providers:\n",
        "- OpenAI (GPT-3.5, GPT-4)\n",
        "- Anthropic (Claude-2)\n",
        "- Meta (Llama-2)\n",
        "- Mistral AI (Mistral-7B)"
      ],
      "metadata": {
        "id": "0ZXdG86VUZlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Required Libraries\n",
        "Import necessary packages for model interaction, data processing, and visualization\n"
      ],
      "metadata": {
        "id": "z8s6WviWUcer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n"
      ],
      "metadata": {
        "id": "Mvw77YEllgak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "R420LJUoT7nB"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import anthropic\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Configuration\n",
        "Define the models to compare and their configurations"
      ],
      "metadata": {
        "id": "lCunXEpjmjNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    def __init__(self, name, api_key=None):\n",
        "        self.name = name\n",
        "        self.api_key = api_key\n",
        "\n",
        "models_to_compare = {\n",
        "    \"gpt-3.5-turbo\": {\"type\": \"api\", \"provider\": \"openai\"},\n",
        "    \"claude-2\": {\"type\": \"api\", \"provider\": \"anthropic\"},\n",
        "    \"microsoft/phi-2\": {\"type\": \"local\", \"provider\": \"huggingface\"},\n",
        "    \"facebook/opt-125m\": {\"type\": \"local\", \"provider\": \"huggingface\"},\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\": {\"type\": \"local\", \"provider\": \"huggingface\"}\n",
        "}"
      ],
      "metadata": {
        "id": "Xp0NNjiDUesA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Cases Definition\n",
        "Define various test cases across different categories to evaluate model performance\n"
      ],
      "metadata": {
        "id": "JyEQor4LmoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"category\": \"General Knowledge\",\n",
        "        \"prompts\": [\n",
        "            \"What is the capital of France?\",\n",
        "            \"Explain quantum computing in simple terms\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Code Generation\",\n",
        "        \"prompts\": [\n",
        "            \"Write a Python function to check if a string is palindrome\",\n",
        "            \"Create a simple REST API using Flask\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Creative Writing\",\n",
        "        \"prompts\": [\n",
        "            \"Write a short story about a time traveler\",\n",
        "            \"Create a poem about artificial intelligence\",\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "8uCsS0IcmrhD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics\n",
        "Define functions to evaluate model responses based on different criteria\n"
      ],
      "metadata": {
        "id": "JyUZNfJhmt3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response(response: str, criteria: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate model response based on different criteria\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"length\": len(response),\n",
        "        \"response_time\": criteria.get(\"response_time\", 0),\n",
        "        # Add more metrics as needed\n",
        "    }\n",
        "    return results"
      ],
      "metadata": {
        "id": "MNhpqiVgmv0U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Wrapper Class\n",
        "Create a wrapper class to handle different types of models consistently\n"
      ],
      "metadata": {
        "id": "WKILrXQVmyPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMWrapper:\n",
        "    def __init__(self, model_name: str, model_config: Dict):\n",
        "        self.model_name = model_name\n",
        "        self.model_config = model_config\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        if self.model_config[\"type\"] == \"api\":\n",
        "            if self.model_config[\"provider\"] == \"openai\":\n",
        "                self.client = openai.OpenAI()\n",
        "            elif self.model_config[\"provider\"] == \"anthropic\":\n",
        "                self.client = anthropic.Anthropic(api_key=self.model_config[\"api_key\"])\n",
        "        else:\n",
        "            try:\n",
        "                # Add device placement for better memory management\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                print(f\"Loading {self.model_name} on {device}\")\n",
        "\n",
        "                # Load with lower precision for memory efficiency\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    token=False  # Explicitly state we're not using token\n",
        "                )\n",
        "                self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    token=False,\n",
        "                    torch_dtype=torch.float16,  # Use float16 for memory efficiency\n",
        "                    low_cpu_mem_usage=True,\n",
        "                    device_map=\"auto\"\n",
        "                )\n",
        "                print(f\"Successfully loaded {self.model_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model {self.model_name}: {e}\")\n",
        "                raise\n",
        "\n",
        "    def generate(self, prompt: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            if self.model_config[\"type\"] == \"api\":\n",
        "                if self.model_config[\"provider\"] == \"openai\":\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                    )\n",
        "                    result = response.choices[0].message.content\n",
        "\n",
        "                elif self.model_config[\"provider\"] == \"anthropic\":\n",
        "                    response = self.client.messages.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                    )\n",
        "                    result = response.content\n",
        "            else:\n",
        "                # Local model inference\n",
        "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "                outputs = self.model.generate(**inputs)\n",
        "                result = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            return {\n",
        "                \"response\": result,\n",
        "                \"response_time\": end_time - start_time,\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"response\": str(e),\n",
        "                \"response_time\": time.time() - start_time,\n",
        "                \"status\": \"error\"\n",
        "            }"
      ],
      "metadata": {
        "id": "_hQoP1VKm2T6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison Class\n",
        "Define class to run comparisons across all models and generate reports\n"
      ],
      "metadata": {
        "id": "BBTtlcCfm4-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelComparison:\n",
        "    def __init__(self, models_config: Dict, openai_key: str, anthropic_key: str):\n",
        "        # Store API keys\n",
        "        self.openai_key = openai_key\n",
        "        self.anthropic_key = anthropic_key\n",
        "\n",
        "        # Initialize models with appropriate API keys\n",
        "        self.models = {}\n",
        "        for name, config in models_config.items():\n",
        "            if config[\"provider\"] == \"openai\":\n",
        "                config[\"api_key\"] = self.openai_key\n",
        "            elif config[\"provider\"] == \"anthropic\":\n",
        "                config[\"api_key\"] = self.anthropic_key\n",
        "            self.models[name] = LLMWrapper(name, config)\n",
        "\n",
        "        self.results = []\n",
        "\n",
        "    def run_comparison(self, test_cases: List[Dict]):\n",
        "        for case in test_cases:\n",
        "            for prompt in case[\"prompts\"]:\n",
        "                for model_name, model in self.models.items():\n",
        "                    result = model.generate(prompt)\n",
        "                    evaluation = evaluate_response(\n",
        "                        result[\"response\"],\n",
        "                        {\"response_time\": result[\"response_time\"]}\n",
        "                    )\n",
        "\n",
        "                    self.results.append({\n",
        "                        \"model\": model_name,\n",
        "                        \"category\": case[\"category\"],\n",
        "                        \"prompt\": prompt,\n",
        "                        \"response\": result[\"response\"],\n",
        "                        \"metrics\": evaluation\n",
        "                    })\n",
        "\n",
        "    def generate_report(self):\n",
        "        # Convert the nested results into a flat DataFrame structure\n",
        "        flat_results = []\n",
        "        for result in self.results:\n",
        "            flat_results.append({\n",
        "                'model': result['model'],\n",
        "                'category': result['category'],\n",
        "                'prompt': result['prompt'],\n",
        "                'response': result['response'],\n",
        "                'response_time': result['metrics']['response_time'],  # Flatten metrics\n",
        "                'response_length': result['metrics']['length']  # Flatten metrics\n",
        "            })\n",
        "        return pd.DataFrame(flat_results)\n",
        "\n",
        "    def plot_comparisons(self):\n",
        "        \"\"\"Plot comparison visualizations with error handling\"\"\"\n",
        "        try:\n",
        "            # Generate the report DataFrame\n",
        "            df = self.generate_report()\n",
        "\n",
        "            if df.empty:\n",
        "                print(\"No data available to plot.\")\n",
        "                return\n",
        "\n",
        "            # Import required libraries\n",
        "            import matplotlib.pyplot as plt\n",
        "            import seaborn as sns\n",
        "\n",
        "            # Set style\n",
        "            plt.style.use('seaborn')\n",
        "\n",
        "            # Create figure with larger size\n",
        "            plt.figure(figsize=(15, 6))\n",
        "\n",
        "            # Response Time Plot\n",
        "            plt.subplot(1, 2, 1)\n",
        "            sns.boxplot(data=df, x='model', y='response_time')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.title('Response Time by Model')\n",
        "            plt.xlabel('Model')\n",
        "            plt.ylabel('Response Time (seconds)')\n",
        "\n",
        "            # Response Length Plot\n",
        "            plt.subplot(1, 2, 2)\n",
        "            sns.barplot(data=df, x='model', y='response_length')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.title('Average Response Length by Model')\n",
        "            plt.xlabel('Model')\n",
        "            plt.ylabel('Response Length (characters)')\n",
        "\n",
        "            # Adjust layout to prevent label cutoff\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Show the plot\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plots: {e}\")\n",
        "            print(\"\\nDebug information:\")\n",
        "            print(f\"Results available: {len(self.results)}\")\n",
        "            if self.results:\n",
        "                print(\"Sample result structure:\", self.results[0].keys())"
      ],
      "metadata": {
        "id": "TqmE8yaPm7Nd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provide API Keys\n",
        "To use this notebook, you'll need API keys from OpenAI and Anthropic. Here's how to get them:\n",
        "\n",
        "1. OpenAI API Key:\n",
        "   - Go to https://platform.openai.com/\n",
        "   - Sign up or log in\n",
        "   - Navigate to Settings > API Keys\n",
        "   - Click \"Create new secret key\"\n",
        "   - Copy the key (you won't be able to see it again)\n",
        "\n",
        "2. Anthropic API Key:\n",
        "   - Go to https://console.anthropic.com/\n",
        "   - Sign up or log in\n",
        "   - Navigate to API Keys section\n",
        "   - Click \"Create Key\"\n",
        "   - Copy the key (starts with 'sk-ant-')\n",
        "\n",
        "⚠️ Security Notes:\n",
        "- Never share your API keys\n",
        "- Keep track of usage/costs\n",
        "- Keys can be revoked and regenerated if compromised\n",
        "- Consider setting up usage limits in the provider dashboards\n"
      ],
      "metadata": {
        "id": "y5sed1KBm9yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Get API keys securely\n",
        "print(\"Enter your OpenAI API Key (input will be hidden):\")\n",
        "openai_key = getpass.getpass()\n",
        "os.environ['OPENAI_API_KEY'] = openai_key\n",
        "\n",
        "print(\"Enter your Anthropic API Key (input will be hidden):\")\n",
        "anthropic_key = getpass.getpass()\n",
        "\n",
        "# Set the keys and verify format\n",
        "if openai_key and anthropic_key:\n",
        "    if not openai_key.startswith('sk-'):\n",
        "        raise ValueError(\"Invalid OpenAI API key format\")\n",
        "    if not anthropic_key.startswith('sk-ant-'):\n",
        "        raise ValueError(\"Invalid Anthropic API key format\")\n",
        "    obsidian://open?vault=Denny&file=GDPR\n",
        "    # Set the keys\n",
        "    openai.api_key = openai_key\n",
        "    anthropic.api_key = anthropic_key\n",
        "    print(\"✅ API keys successfully configured!\")\n",
        "else:\n",
        "    print(\"⚠️ Please enter both API keys to proceed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6R3Ui1ioaE_",
        "outputId": "2fb3bbac-3417-4ff3-cfec-44ee19d92fd7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API Key (input will be hidden):\n",
            "··········\n",
            "Enter your Anthropic API Key (input will be hidden):\n",
            "··········\n",
            "✅ API keys successfully configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison with API keys\n",
        "comparison = ModelComparison(\n",
        "    models_config=models_to_compare,\n",
        "    openai_key=openai.api_key,\n",
        "    anthropic_key=anthropic.api_key\n",
        ")\n",
        "\n",
        "# Run comparison\n",
        "comparison.run_comparison(test_cases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "3d8ae2f30b9d4b46a0878413dbca1fc0",
            "5fe6165195e649768f9a073fa3d29bc3",
            "859fbbb46a734f4da99c3e12c052322f",
            "f028385da6764b5f94e525bf29b38a4f",
            "ba378fa0d2564d2199efc73723aaeeb3",
            "8f767e29ecc34829936ff2d590054d2f",
            "68798ad0d08e4fecb46e7f8d15aedf94",
            "16b145075f724e02ba8def5ac38f0085",
            "e7a5b867feb64e80be28f7c5650cda79",
            "dc9f1506acfe42b2b668d11f63b25674",
            "72811a747a6c458aa27cc1c4c57f6213"
          ]
        },
        "id": "rbv7nCmNm_fK",
        "outputId": "291ab922-4902-4947-eecf-c7414539df26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading microsoft/phi-2 on cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d8ae2f30b9d4b46a0878413dbca1fc0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded microsoft/phi-2\n",
            "Loading facebook/opt-125m on cpu\n",
            "Successfully loaded facebook/opt-125m\n",
            "Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0 on cpu\n",
            "Successfully loaded TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Display Results\n",
        "Create reports and visualizations from the comparison results\n"
      ],
      "metadata": {
        "id": "IyuUYAXGnecL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and display results\n",
        "comparison.generate_report()\n",
        "comparison.plot_comparisons()\n"
      ],
      "metadata": {
        "id": "tVWiiFwdng5s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}