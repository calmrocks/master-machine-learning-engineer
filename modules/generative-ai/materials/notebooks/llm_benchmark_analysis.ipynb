{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/modules/generative-ai/materials/notebooks/llm_benchmark_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Foundation Models Comparison\n",
        "\n",
        "This notebook compares different LLM foundation models across various aspects including:\n",
        "- Response quality\n",
        "- Performance metrics\n",
        "- Resource usage\n",
        "- Practical considerations\n",
        "\n",
        "We'll test models from different providers:\n",
        "- OpenAI (GPT-3.5, GPT-4)\n",
        "- Anthropic (Claude-2)\n",
        "- Meta (Llama-2)\n",
        "- Mistral AI (Mistral-7B)"
      ],
      "metadata": {
        "id": "0ZXdG86VUZlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Required Libraries\n",
        "Import necessary packages for model interaction, data processing, and visualization\n"
      ],
      "metadata": {
        "id": "z8s6WviWUcer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n"
      ],
      "metadata": {
        "id": "Mvw77YEllgak",
        "outputId": "7e17101f-5150-4708-a25d-5d8d42634202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/243.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/243.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R420LJUoT7nB"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import anthropic\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Configuration\n",
        "Define the models to compare and their configurations"
      ],
      "metadata": {
        "id": "lCunXEpjmjNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    def __init__(self, name, api_key=None):\n",
        "        self.name = name\n",
        "        self.api_key = api_key\n",
        "\n",
        "models_to_compare = {\n",
        "    \"gpt-3.5-turbo\": {\"type\": \"api\", \"provider\": \"openai\"},\n",
        "    \"gpt-4\": {\"type\": \"api\", \"provider\": \"openai\"},\n",
        "    \"claude-2\": {\"type\": \"api\", \"provider\": \"anthropic\"},\n",
        "    \"llama-2-7b\": {\"type\": \"local\", \"provider\": \"meta\"},\n",
        "    \"mistral-7b\": {\"type\": \"local\", \"provider\": \"mistral\"}\n",
        "}"
      ],
      "metadata": {
        "id": "Xp0NNjiDUesA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Cases Definition\n",
        "Define various test cases across different categories to evaluate model performance\n"
      ],
      "metadata": {
        "id": "JyEQor4LmoUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"category\": \"General Knowledge\",\n",
        "        \"prompts\": [\n",
        "            \"What is the capital of France?\",\n",
        "            \"Explain quantum computing in simple terms\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Code Generation\",\n",
        "        \"prompts\": [\n",
        "            \"Write a Python function to check if a string is palindrome\",\n",
        "            \"Create a simple REST API using Flask\",\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Creative Writing\",\n",
        "        \"prompts\": [\n",
        "            \"Write a short story about a time traveler\",\n",
        "            \"Create a poem about artificial intelligence\",\n",
        "        ]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "8uCsS0IcmrhD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics\n",
        "Define functions to evaluate model responses based on different criteria\n"
      ],
      "metadata": {
        "id": "JyUZNfJhmt3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response(response: str, criteria: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate model response based on different criteria\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"length\": len(response),\n",
        "        \"response_time\": criteria.get(\"response_time\", 0),\n",
        "        # Add more metrics as needed\n",
        "    }\n",
        "    return results"
      ],
      "metadata": {
        "id": "MNhpqiVgmv0U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Wrapper Class\n",
        "Create a wrapper class to handle different types of models consistently\n"
      ],
      "metadata": {
        "id": "WKILrXQVmyPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMWrapper:\n",
        "    def __init__(self, model_name: str, model_config: Dict):\n",
        "        self.model_name = model_name\n",
        "        self.model_config = model_config\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        if self.model_config[\"type\"] == \"api\":\n",
        "            if self.model_config[\"provider\"] == \"openai\":\n",
        "                self.client = openai.OpenAI()\n",
        "            elif self.model_config[\"provider\"] == \"anthropic\":\n",
        "                self.client = anthropic.Anthropic()\n",
        "        else:\n",
        "            # Load local models\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
        "\n",
        "    def generate(self, prompt: str) -> Dict:\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            if self.model_config[\"type\"] == \"api\":\n",
        "                if self.model_config[\"provider\"] == \"openai\":\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                    )\n",
        "                    result = response.choices[0].message.content\n",
        "\n",
        "                elif self.model_config[\"provider\"] == \"anthropic\":\n",
        "                    response = self.client.messages.create(\n",
        "                        model=self.model_name,\n",
        "                        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "                    )\n",
        "                    result = response.content\n",
        "            else:\n",
        "                # Local model inference\n",
        "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "                outputs = self.model.generate(**inputs)\n",
        "                result = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            return {\n",
        "                \"response\": result,\n",
        "                \"response_time\": end_time - start_time,\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"response\": str(e),\n",
        "                \"response_time\": time.time() - start_time,\n",
        "                \"status\": \"error\"\n",
        "            }"
      ],
      "metadata": {
        "id": "_hQoP1VKm2T6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison Class\n",
        "Define class to run comparisons across all models and generate reports\n"
      ],
      "metadata": {
        "id": "BBTtlcCfm4-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelComparison:\n",
        "    def __init__(self, models_config: Dict):\n",
        "        self.models = {\n",
        "            name: LLMWrapper(name, config)\n",
        "            for name, config in models_config.items()\n",
        "        }\n",
        "        self.results = []\n",
        "\n",
        "    def run_comparison(self, test_cases: List[Dict]):\n",
        "        for case in test_cases:\n",
        "            for prompt in case[\"prompts\"]:\n",
        "                for model_name, model in self.models.items():\n",
        "                    result = model.generate(prompt)\n",
        "                    evaluation = evaluate_response(\n",
        "                        result[\"response\"],\n",
        "                        {\"response_time\": result[\"response_time\"]}\n",
        "                    )\n",
        "\n",
        "                    self.results.append({\n",
        "                        \"model\": model_name,\n",
        "                        \"category\": case[\"category\"],\n",
        "                        \"prompt\": prompt,\n",
        "                        \"response\": result[\"response\"],\n",
        "                        \"metrics\": evaluation\n",
        "                    })\n",
        "\n",
        "    def generate_report(self):\n",
        "        df = pd.DataFrame(self.results)\n",
        "        return df\n",
        "\n",
        "    def plot_comparisons(self):\n",
        "        df = self.generate_report()\n",
        "\n",
        "        # Response time comparison\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        df.boxplot(column=\"metrics.response_time\", by=\"model\")\n",
        "        plt.title(\"Response Time Comparison\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "TqmE8yaPm7Nd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute Comparison\n",
        "Run the comparison across all models and test cases\n"
      ],
      "metadata": {
        "id": "y5sed1KBm9yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparison = ModelComparison(models_to_compare)\n",
        "comparison.run_comparison(test_cases)\n"
      ],
      "metadata": {
        "id": "rbv7nCmNm_fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Display Results\n",
        "Create reports and visualizations from the comparison results\n"
      ],
      "metadata": {
        "id": "IyuUYAXGnecL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate report\n",
        "results_df = comparison.generate_report()\n",
        "\n",
        "# Display results\n",
        "print(\"Summary Statistics:\")\n",
        "print(results_df.groupby('model')['metrics.response_time'].describe())\n",
        "\n",
        "# Plot comparisons\n",
        "comparison.plot_comparisons()\n",
        "\n",
        "# Export results\n",
        "results_df.to_csv(\"llm_comparison_results.csv\")"
      ],
      "metadata": {
        "id": "tVWiiFwdng5s",
        "outputId": "a71bad9b-722e-4a32-d95b-109a5cf8efe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'comparison' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-165791afa03a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary Statistics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'comparison' is not defined"
          ]
        }
      ]
    }
  ]
}