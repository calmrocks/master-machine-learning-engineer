{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/GenAI/BasicLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Small Language Model in Google Colab\n",
        "\n",
        "## Import Libraries\n",
        "First, we need to install the required libraries:"
      ],
      "metadata": {
        "id": "FY6Aoi7GYvV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU5JZ6Rq07cM",
        "outputId": "1799b6e7-8e61-4cba-eed0-9e1417e93748"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7B4lhNnxYKNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa7c7b3-f4e2-46b8-8c5e-48274d08126a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.utils.data import DataLoader\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "dWrNzmVQ4LgQ",
        "outputId": "8fba9f27-6961-4bc4-8fca-ba27a1d81075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Device (CPU/GPU)"
      ],
      "metadata": {
        "id": "fKguzCEZw7lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woykbDFhY3rg",
        "outputId": "e02d6bae-efef-4542-f2e1-c674e758eea8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Hyperparameters"
      ],
      "metadata": {
        "id": "7IL0dgcnZL9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128    # Embedding dimension\n",
        "num_heads = 4      # Number of attention heads\n",
        "num_layers = 2     # Number of transformer layers\n",
        "ffn_hidden_dim = 512  # Feed-forward network hidden dimension\n",
        "seq_length = 20    # Maximum sequence length\n",
        "batch_size = 32    # Batch size for training\n",
        "num_epochs = 10    # Number of training epochs"
      ],
      "metadata": {
        "id": "TPxQ4euMZI4k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "IQK0U8LiZUYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = nltk.word_tokenize\n",
        "\n",
        "# Function to yield data, tokenized\n",
        "def yield_tokens(data_iter):\n",
        "    for item in data_iter:\n",
        "        yield tokenizer(item['text'])\n",
        "\n",
        "# Load AG_NEWS dataset\n",
        "dataset = load_dataset('ag_news')\n",
        "\n",
        "# Build vocabulary\n",
        "texts = [\" \".join(tokenizer(item['text'])) for item in dataset['train']]\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(texts)\n",
        "vocab = vectorizer.vocabulary_\n",
        "vocab_size = len(vocab) + 1  # +1 for <unk> token\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Text processing function\n",
        "def text_to_indices(text):\n",
        "    tokens = tokenizer(text)\n",
        "    indices = [vocab.get(token, vocab_size - 1) for token in tokens]  # vocab_size - 1 is the index for <unk>\n",
        "    return indices\n",
        "\n",
        "def collate_batch(batch):\n",
        "    labels, texts = [], []\n",
        "    for item in batch:\n",
        "        labels.append(item['label'])\n",
        "        processed_text = text_to_indices(item['text'])\n",
        "        texts.append(processed_text)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    texts = nn.utils.rnn.pad_sequence([torch.tensor(text, dtype=torch.long) for text in texts], padding_value=vocab_size - 1)\n",
        "    return labels, texts\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_kHb75tZQTi",
        "outputId": "3bd3495e-a936-4587-f69f-97c81d1ab635"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 65004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Transformer Model"
      ],
      "metadata": {
        "id": "Rq8kR4IyZb6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ffn_hidden_dim, seq_length):\n",
        "        super(SmallTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # positional_encoding is no longer a Parameter, but is created dynamically in forward\n",
        "        # self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ffn_hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Create positional encoding dynamically based on the input sequence length\n",
        "        # src.size(1) gives the actual sequence length of the input\n",
        "        positional_encoding = torch.zeros(1, src.size(1), self.embedding.embedding_dim, device=src.device)\n",
        "        # use register_buffer to make it a non-trainable parameter, moved to the correct device\n",
        "\n",
        "        embedded = self.embedding(src) + positional_encoding\n",
        "        encoded = self.transformer_encoder(embedded)\n",
        "        output = self.fc_out(encoded)\n",
        "        return output\n",
        "\n",
        "model = SmallTransformer(vocab_size, embed_dim, num_heads, num_layers, ffn_hidden_dim, seq_length).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "caz1_MC_ZZHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d668e1c1-613b-442a-b27e-071b29a056f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SmallTransformer(\n",
            "  (embedding): Embedding(65004, 128)\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc_out): Linear(in_features=128, out_features=65004, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "_B-g556wZiaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "czNeghXWZfOg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "VXrG-yN8Zr5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for labels, texts in train_dataloader:\n",
        "        labels = labels.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(texts)\n",
        "        output = output.view(-1, vocab_size)\n",
        "        texts = texts.view(-1)\n",
        "\n",
        "        loss = criterion(output, texts)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    train_losses.append(avg_loss)"
      ],
      "metadata": {
        "id": "xtSm97_cZrHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation (Optional)"
      ],
      "metadata": {
        "id": "ki390pn9Zzo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_eval_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for labels, texts in test_dataloader:\n",
        "        labels = labels.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        output = model(texts)\n",
        "        output = output.view(-1, vocab_size)\n",
        "        texts = texts.view(-1)\n",
        "\n",
        "        loss = criterion(output, texts)\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "avg_eval_loss = total_eval_loss / len(test_dataloader)\n",
        "print(f'Evaluation Loss: {avg_eval_loss:.4f}')"
      ],
      "metadata": {
        "id": "4XgPVhvgZxD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Training Loss"
      ],
      "metadata": {
        "id": "c3VNT8iTZ5TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4t_0ACOlZ3Gd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}