{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNr/G8FYqbD887pZhNRuGlh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calmrocks/master-machine-learning-engineer/blob/main/GenAI/PromptEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Prompt Engineering with Falcon-7B\n",
        "This notebook demonstrates various prompt engineering techniques using the Falcon-7B model. We'll explore different prompting strategies and parameter tuning."
      ],
      "metadata": {
        "id": "bPsHOzw3CRvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Practices\n",
        "Key takeaways:\n",
        "1. Zero-shot works best for simple, straightforward tasks\n",
        "2. Chain of Thought is excellent for complex reasoning\n",
        "3. Few-shot is ideal when you have specific examples\n",
        "4. Tree of Thoughts helps with decision-making tasks\n",
        "\n",
        "Best practices:\n",
        "- Start simple and increase complexity as needed\n",
        "- Match the technique to the task type\n",
        "- Consider computational efficiency\n",
        "- Test different temperature values for optimal results"
      ],
      "metadata": {
        "id": "qSdYekyEGJKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9Lg8erBnAr5e"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q transformers accelerate\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# Memory management utilities\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared\")\n",
        "\n",
        "def check_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Used: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
        "        print(f\"Cached: {torch.cuda.memory_reserved()/1e9:.2f}GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup\n",
        "We'll use Falcon-7B-Instruct, a powerful 7B parameter model fine-tuned for instruction following. To run it on Colab's T4 GPU (16GB VRAM), we'll use 4-bit quantization to reduce memory usage."
      ],
      "metadata": {
        "id": "BHXqh0QACd_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_model(temperature=0.7, top_p=0.9):\n",
        "    model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "    print(\"Loading Falcon-7B model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True  # Add this line\n",
        "    )\n",
        "\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    return generator\n",
        "\n",
        "# Initialize default model\n",
        "try:\n",
        "    generator = setup_model()\n",
        "    check_gpu_memory()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "z59VbPD2BOJD",
        "outputId": "bac667e3-b2f8-44a9-ef4f-34d7b5918c75"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Falcon-7B model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-738a5d806626>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Initialize default model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mcheck_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-738a5d806626>\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(temperature, top_p)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Falcon-7B model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3620\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3621\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Parameters\n",
        "Let's explore how different parameters affect the model's output:\n",
        "- Temperature: Controls randomness (higher = more creative, lower = more focused)\n",
        "- Top-p: Controls diversity via nucleus sampling\n",
        "- Max tokens: Controls response length"
      ],
      "metadata": {
        "id": "1o7jcaNRHME6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_params(prompt, temperature=0.7, top_p=0.9, max_tokens=256):\n",
        "    generator = setup_model(temperature=temperature, top_p=top_p)\n",
        "\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    clear_gpu_memory()  # Clean up after generation\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# Example: Temperature comparison\n",
        "prompt = \"Write a short story about a robot learning to paint:\"\n",
        "\n",
        "print(\"Conservative (Temperature = 0.3):\")\n",
        "print(generate_with_params(prompt, temperature=0.3))\n",
        "print(\"\\nBalanced (Temperature = 0.7):\")\n",
        "print(generate_with_params(prompt, temperature=0.7))\n",
        "print(\"\\nCreative (Temperature = 1.2):\")\n",
        "print(generate_with_params(prompt, temperature=1.2))"
      ],
      "metadata": {
        "id": "tt0jtpcoHM-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Zero-shot Prompting\n",
        "Zero-shot prompting is the simplest form of prompting where we directly ask the model to perform a task without providing any examples. This technique tests the model's ability to understand and perform tasks based solely on instructions.\n",
        "\n",
        "Key characteristics:\n",
        "- No examples provided\n",
        "- Relies on model's pre-trained knowledge\n",
        "- Simplest to implement but may not always give optimal results"
      ],
      "metadata": {
        "id": "YMMrEIacEq5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Zero-shot Prompting Example\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify the sentiment of this text as positive, negative, or neutral:\n",
        "Text: \"I absolutely love this new phone! It's amazing!\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "response = generate_response(generator, zero_shot_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2wS28xmBpd3",
        "outputId": "fc852167-bf2c-44a0-c8ff-470d8b85da52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-shot Prompting Example\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classify the sentiment of this text as positive, negative, or neutral:\n",
            "Text: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: \"I absolutely love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. One-shot Prompting\n",
        "One-shot prompting provides the model with a single example of the desired task before asking it to perform a similar task. This can help the model better understand the expected format and type of response.\n",
        "\n",
        "Benefits:\n",
        "- Provides context through a single example\n",
        "- Better performance than zero-shot\n",
        "- Still maintains simplicity"
      ],
      "metadata": {
        "id": "sEAITWkeFwun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"One-shot Prompting Example\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "one_shot_prompt = \"\"\"Classify the sentiment of text as positive, negative, or neutral:\n",
        "\n",
        "Text: \"This movie was terrible, I hated it.\"\n",
        "Sentiment: negative\n",
        "\n",
        "Text: \"I absolutely love this new phone! It's amazing!\"\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "response = generate_response(generator, one_shot_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pTPuI-TE2MD",
        "outputId": "afadbe51-6aee-498a-e35d-6c0bc6a6f5c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-shot Prompting Example\n",
            "--------------------------------------------------\n",
            "Classify the sentiment of text as positive, negative, or neutral:\n",
            "\n",
            "Text: \"This movie was terrible, I hated it.\"\n",
            "Sentiment: negative\n",
            "\n",
            "Text: \"I absolutely love this new phone! It's amazing!\"\n",
            "Sentiment: positive\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: neutral\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: positive\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: neutral\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: positive\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: neutral\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: neutral\n",
            "\n",
            "Text: \"I love this movie, it's so funny!\"\n",
            "Sentiment: neutral\n",
            "\n",
            "Text: \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Few-shot Prompting\n",
        "Few-shot prompting extends one-shot prompting by providing multiple examples. This technique helps the model better understand patterns and expectations through multiple demonstrations.\n",
        "\n",
        "Advantages:\n",
        "- More robust performance\n",
        "- Better pattern recognition\n",
        "- Clearer context for complex tasks"
      ],
      "metadata": {
        "id": "XUarnlxfF3m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Few-shot Prompting\n",
        "print(\"Few-shot Prompting Example\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "few_shot_prompt = \"\"\"Solve the following math problems:\n",
        "\n",
        "Problem: What is 15% of 200?\n",
        "Solution: Let's solve this step by step:\n",
        "1. To find 15% of 200, multiply 200 by 15/100\n",
        "2. 200 × 15/100 = 200 × 0.15 = 30\n",
        "Answer: 30\n",
        "\n",
        "Problem: What is 25% of 80?\n",
        "Solution: Let's solve this step by step:\"\"\"\n",
        "\n",
        "response = generate_response(generator, few_shot_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "vWf4oFIBFT_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Chain of Thought (CoT) Prompting\n",
        "print(\"Chain of Thought Example\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "cot_prompt = \"\"\"Let's solve this word problem step by step:\n",
        "\n",
        "Problem: If a store has 150 apples and sells 30% of them on Monday, then sells 40% of the remaining apples on Tuesday, how many apples are left?\n",
        "\n",
        "Let's think about this step by step:\n",
        "1. First, let's calculate how many apples are sold on Monday\n",
        "   * 30% of 150 = 150 × 0.30 = 45 apples sold\n",
        "   * Remaining after Monday = 150 - 45 = 105 apples\n",
        "\n",
        "2. Next, let's calculate Tuesday's sales\n",
        "   * 40% of 105 = 105 × 0.40 = 42 apples sold\n",
        "   * Remaining after Tuesday = 105 - 42 = 63 apples\n",
        "\n",
        "Therefore, there are 63 apples left.\n",
        "\n",
        "Problem: A restaurant has 200 customers per day and 15% are breakfast customers, 45% are lunch customers, and the rest are dinner customers. How many dinner customers are there?\n",
        "\n",
        "Let's think about this step by step:\"\"\"\n",
        "\n",
        "response = generate_response(generator, cot_prompt, max_length=400)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "VaSolv57FWCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: RAG (Retrieval-Augmented Generation) Example\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "# Setup knowledge base\n",
        "knowledge_base = \"\"\"\n",
        "Python is a high-level, interpreted programming language created by Guido van Rossum in 1991.\n",
        "Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming.\n",
        "Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize ChromaDB and embed knowledge\n",
        "chroma_client = chromadb.Client()\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction()\n",
        "\n",
        "# Create collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=\"python_facts\",\n",
        "    embedding_function=sentence_transformer_ef\n",
        ")\n",
        "\n",
        "# Add documents\n",
        "collection.add(\n",
        "    documents=[knowledge_base],\n",
        "    metadatas=[{\"source\": \"python_docs\"}],\n",
        "    ids=[\"doc1\"]\n",
        ")\n",
        "\n",
        "def rag_response(query, collection, generator):\n",
        "    # Retrieve relevant information\n",
        "    results = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=1\n",
        "    )\n",
        "\n",
        "    # Construct prompt with retrieved information\n",
        "    context = results['documents'][0][0]\n",
        "    prompt = f\"\"\"Using the following information, answer the question.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return generate_response(generator, prompt)\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Python and who created it?\"\n",
        "response = rag_response(query, collection, generator)\n",
        "print(\"RAG Example\")\n",
        "print(\"-\" * 50)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "WemdtXTGFbZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Instruction-Oriented Prompting (IoT)\n",
        "print(\"Instruction-Oriented Prompting Example\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "iot_prompt = \"\"\"Instructions: Generate a professional email to reschedule a meeting. Use these guidelines:\n",
        "- Be polite and professional\n",
        "- Provide a reason for rescheduling\n",
        "- Suggest two alternative times\n",
        "- Ask for confirmation\n",
        "\n",
        "Email:\"\"\"\n",
        "\n",
        "response = generate_response(generator, iot_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "A6ca7EDMFegt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Tree of Thoughts (ToT) Prompting\n",
        "print(\"Tree of Thoughts Example\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "tot_prompt = \"\"\"Problem: Plan a birthday party for a 10-year-old child.\n",
        "\n",
        "Let's explore multiple thought paths:\n",
        "\n",
        "Path 1 - Indoor Party:\n",
        "1. Venue options:\n",
        "   * Home party\n",
        "   * Indoor playground\n",
        "   * Party center\n",
        "2. Activities:\n",
        "   * Games\n",
        "   * Crafts\n",
        "   * Entertainment\n",
        "\n",
        "Path 2 - Outdoor Party:\n",
        "1. Venue options:\n",
        "   * Park\n",
        "   * Backyard\n",
        "   * Sports facility\n",
        "2. Activities:\n",
        "   * Sports\n",
        "   * Outdoor games\n",
        "   * Nature activities\n",
        "\n",
        "Let's evaluate each path and choose the best option considering:\n",
        "1. Weather risks\n",
        "2. Cost\n",
        "3. Entertainment value\n",
        "4. Practicality\n",
        "\n",
        "Analysis:\"\"\"\n",
        "\n",
        "response = generate_response(generator, tot_prompt, max_length=500)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "pLEb5_CpFjKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}